"""
üéØ PredictiveDialogAgent - Êô∫ËÉΩÂØπËØù‰ª£ÁêÜÊ†∏ÂøÉÁ≥ªÁªü
ÊäÄÊúØÊû∂ÊûÑÔºöÈ¢ÑÊµãÁºñÁ†Å(Predictive Coding) + Ê∑±Â∫¶ËÆ∞ÂøÜÁÆ°ÁêÜ + DeepSeekÂ§ßÊ®°ÂûãÈõÜÊàê
ÂäüËÉΩ‰∫ÆÁÇπÔºö
1. Ëá™ÈÄÇÂ∫îËÆ∞ÂøÜËß¶ÂèëÊú∫Âà∂
2. ÊÄùÁª¥Èìæ(CoT)ÂèØËßÜÂåñÁÆ°ÁêÜ
3. Âä®ÊÄÅ‰∏âÂÖÉÁªÑÊäΩÂèñ
4. ËÆ∞ÂøÜÊåÅ‰πÖÂåñÂ≠òÂÇ®
5. ÂÆûÊó∂È¢ÑÊµãËØØÂ∑ÆÁõëÊéß
"""
import os
import torch

# ËÆæÁΩÆÈïúÂÉèÊ∫êÔºàÊîØÊåÅHTTPÂçèËÆÆÈÅøÂÖçSSLÈóÆÈ¢òÔºâ
os.environ["HF_ENDPOINT"] = "http://hf-mirror.com"  # ÂõΩÂÜÖÈïúÂÉèÊ∫ê[5](@ref)

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

from openai import AsyncOpenAI
from collections import deque
from typing import List, Tuple, Dict, Union, Any
import asyncio
import json
import hashlib
import traceback # For debugging recall

# --------------- Ê†∏ÂøÉÂØπËØù‰ª£ÁêÜÂÆûÁé∞ ---------------
class DialogHistoryBuffer:
    """ÂØπËØùÂéÜÂè≤‰∏éÊÄùÁª¥ÈìæÁÆ°ÁêÜ"""
    def __init__(self, max_len=10):
        self.history = deque(maxlen=max_len)
        self.thought_chain = []

    def add_dialog(self, user_input: str, agent_response: str):
        self.history.append(("user", user_input))
        self.history.append(("assistant", agent_response))

    def add_thought_step(self, thought: str):
        self.thought_chain.append(thought)
        if len(self.thought_chain) > 5:
            self.thought_chain.pop(0)

    def context_text(self) -> str:
        """Ê†ºÂºèÂåñÂØπËØùÂéÜÂè≤"""
        return "\n".join([f"{'User' if role=='user' else 'AI'}: {text}" 
                         for role, text in self.history])

    def chain_text(self) -> str:
        """Ê†ºÂºèÂåñÊÄùÁª¥Èìæ"""
        return "\n".join([f"Step {i+1}: {thought}" 
                         for i, thought in enumerate(self.thought_chain)])

    def clear_chain(self):
        self.thought_chain.clear()

class PredictiveDialogAgent:
    """
    È¢ÑÊµãÁºñÁ†ÅÂØπËØù‰ª£ÁêÜ‰∏ªÁ±ª
    Refactored to use a unified memory system managed by IntegratedSystem.
    """
    def __init__(self, 
                 deepseek_api_key: str,
                 integrated_system_ref: Any, # Reference to the IntegratedSystem instance
                 tool_threshold: float = 0.25,
                 memory_threshold: float = 0.15): # memory_threshold might be re-evaluated
        
        self.integrated_system = integrated_system_ref # Store reference to IntegratedSystem

        # Ê†∏ÂøÉÁªÑ‰ª∂ÂàùÂßãÂåñ
        self.embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2') # May still be used for some local text processing
        self.client = AsyncOpenAI(
            base_url="https://api.deepseek.com/v1",
            api_key=deepseek_api_key
        )
        
       
        self.current_prediction_error = 0.1 # Default placeholder value

        # ÂØπËØùÁÆ°ÁêÜÁ≥ªÁªü
        self.dialog_buffer = DialogHistoryBuffer(max_len=8)
        
        # ÈòàÂÄºËÆæÁΩÆ
        self.tool_trigger_threshold = tool_threshold
        # memory_threshold will need to be re-evaluated in context of new predictive coding logic
        self.memory_trigger_threshold = memory_threshold 
        
       
    
    async def extract_triplet(self, text: str) -> Union[Tuple[str, str, str], None]:
        """‰ΩøÁî®Â§ßÊ®°ÂûãÊäΩÂèñÁü•ËØÜ‰∏âÂÖÉÁªÑ"""
        PROMPT = (
            "ËØ∑‰ªé‰∏ãÈù¢ÁöÑÂØπËØùÂÜÖÂÆπ‰∏≠ÊäΩÂèñÊ†∏ÂøÉÁü•ËØÜ‰∏âÂÖÉÁªÑ(‰∏ª‰Ωì, ÂÖ≥Á≥ª, ÂÆ¢‰Ωì)„ÄÇ"
            "Ê†ºÂºèË¶ÅÊ±ÇÔºö‰∏•Ê†ºÊåâ[‰∏ª‰Ωì, ÂÖ≥Á≥ª, ÂÆ¢‰Ωì]ËøîÂõûÔºå‰∏çË¶ÅËß£Èáä„ÄÇ"
            f"ÂØπËØùÂÜÖÂÆπÔºö{text}"
        )
        
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "‰Ω†ÊòØÊúâ‰∏âÈáçËí∏È¶èËÉΩÂäõÁöÑÁü•ËØÜÊäΩÂèñ‰∏ìÂÆ∂"},
                    {"role": "user", "content": PROMPT}
                ]
            )
            result = response.choices[0].message.content.strip()
            
            # Ëß£Êûê‰∏âÂÖÉÁªÑÊ†ºÂºè
            if result.startswith("[") and result.endswith("]"):
                items = result[1:-1].split(",")
                if len(items) == 3:
                    return tuple(i.strip() for i in items)
        except Exception as e:
            print(f"[ERROR] Triplet extraction failed: {str(e)}")
        return None

    async def generate_response(self, prompt: str) -> str:
        """ÁîüÊàêÂØπËØùÂìçÂ∫îÁöÑÊ†∏ÂøÉÈÄªËæë"""
        print(f"DEBUG_RECALL: PDA.generate_response called with prompt: '{prompt}'")
        # 1. ‰∏ä‰∏ãÊñáÊûÑÂª∫
        dialog_ctx = self.dialog_buffer.context_text()
        thought_chain = self.dialog_buffer.chain_text() or "È¶ñÊ¨°ÊÄùËÄÉË∑ØÂæÑ"
        
        # 2. ËÆ∞ÂøÜÊ£ÄÁ¥¢ (using Core Unified Memory via IntegratedSystem)
        memory_block = "Êó†Áõ∏ÂÖ≥Ê†∏ÂøÉÁü•ËØÜÂ∫ìËÆ∞ÂøÜ" # Default if no relevant info found
        query_vector_for_kb = None

        print(f"DEBUG_RECALL: Attempting to get query vector from IntegratedSystem for prompt: '{prompt}'")
        try:
            query_vector_for_kb = await self.integrated_system.get_tn_query_vector(prompt)
        except Exception as e_qv:
            print(f"DEBUG_RECALL: Error calling get_tn_query_vector: {e_qv}")
            traceback.print_exc()

        if query_vector_for_kb is not None:
            print(f"DEBUG_RECALL: Successfully got query_vector_for_kb. Shape: {query_vector_for_kb.shape}, First 3 elements: {query_vector_for_kb[:3] if query_vector_for_kb.numel() > 0 else 'empty tensor'}")
            # Query the core knowledge base
            # query_core_knowledge_base expects query_vector to be on CPU for its internal logic
            print(f"DEBUG_RECALL: Calling query_core_knowledge_base with the obtained vector.")
            retrieved_items = []
            try:
                retrieved_items = self.integrated_system.query_core_knowledge_base(
                    query_vector=query_vector_for_kb.cpu(),
                    top_k=3
                )
            except Exception as e_qkb:
                print(f"DEBUG_RECALL: Error calling query_core_knowledge_base: {e_qkb}")
                traceback.print_exc()

            print(f"DEBUG_RECALL: Retrieved_items from KB: {retrieved_items}")
            if retrieved_items:
                formatted_memory_results = []
                for triple_key_str, _, score in retrieved_items:
                    try:
                        triple_tuple = tuple(json.loads(triple_key_str))
                        readable_triple = f"({triple_tuple[0]}, {triple_tuple[1]}, {triple_tuple[2]})"
                        formatted_memory_results.append(f"- {readable_triple} (Áõ∏‰ººÂ∫¶: {score:.3f})")
                    except json.JSONDecodeError:
                        formatted_memory_results.append(f"- {triple_key_str} (Áõ∏‰ººÂ∫¶: {score:.3f})") # Fallback to raw key
                if formatted_memory_results:
                    memory_block = "\n".join(formatted_memory_results)
                else:
                    memory_block = "Ê†∏ÂøÉÁü•ËØÜÂ∫ìÊ£ÄÁ¥¢Âà∞ÂÜÖÂÆπÔºå‰ΩÜÊ†ºÂºèÂåñÂ§±Ë¥•„ÄÇ"
                print(f"DEBUG_RECALL: Formatted memory_block: {memory_block}")
            else:
                memory_block = "Ê†∏ÂøÉÁü•ËØÜÂ∫ìÊú™Ê£ÄÁ¥¢Âà∞Áõ∏ÂÖ≥ËÆ∞ÂøÜ„ÄÇ"
                print(f"DEBUG_RECALL: No items retrieved from core knowledge base.")
        else:
            memory_block = "Êó†Ê≥ï‰∏∫ÂΩìÂâçËæìÂÖ•ÁîüÊàêÊü•ËØ¢ÂêëÈáè (get_tn_query_vector returned None)"
            print(f"DEBUG_RECALL: query_vector_for_kb is None. Memory block set to: '{memory_block}'")

        prediction_error = self.current_prediction_error 
        print(f"DEBUG_RECALL: Using prediction_error: {prediction_error:.3f} for LLM prompt.")
        
        # 3. ÊûÑÂª∫Á≥ªÁªüÊèêÁ§∫ÔºàÂ§öÈáçËÆ§Áü•Êù•Ê∫êÊï¥ÂêàÔºâ
        SYSTEM_PROMPT = f"""
# ËßíËâ≤ËÆæÂÆö
‰Ω†ÊòØÊ≠åËïæËíÇÂ®Ö,ÂÆûÈ™åÂÆ§ÁöÑÁßëÁ†îÂä©ÊâãÔºåÊã•ÊúâË∑®Â≠¶ÁßëÁü•ËØÜÂíåÂÆûÈ™åÊï∞ÊçÆÂàÜÊûêËÉΩÂäõ„ÄÇ

# ÂØπËØùÂéÜÂè≤
{dialog_ctx}

# ÊÄùÁª¥ÈìæËÆ∞ÂΩï
{thought_chain}

# ÂÖ≥ËÅîËÆ∞ÂøÜ
{memory_block}

# ËÆ§Áü•Áä∂ÊÄÅ
È¢ÑÊµãËØØÂ∑Æ: {prediction_error:.3f} | {'ÈúÄË¶ÅÊ∑±Â∫¶ÊÄùËÄÉ' if prediction_error > 0.2 else 'Â∏∏ËßÑÂ∫îÁ≠îÊ®°Âºè'}

# ÂìçÂ∫îË¶ÅÊ±Ç
1. ‰∏•Ê†º‰øùÊåÅ‰∏ì‰∏ö‰∏•Ë∞®ÁöÑÂ≠¶ÊúØÁî®ËØ≠‰π†ÊÉØ
2. ÂØπÂ§çÊùÇÊ¶ÇÂøµÊèê‰æõÁÆÄÊòéËÉåÊôØËß£Èáä
3. Ê†∏ÂøÉÂõûÁ≠îÊéßÂà∂Âú®200Â≠óÂÜÖ
4. ÁªìÂ∞æÊ†áÊ≥®[ÊÄùËÄÉÂÆåÊàê]
"""
        # 4. Ë∞ÉÁî®DeepSeek APIÁîüÊàêÂìçÂ∫î
        try:
            print(f"DEBUG_RECALL: Calling LLM with System Prompt including memory_block: \n---\n{memory_block}\n---")
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.35,
                max_tokens=512,
                stream=True
            )
            
            full_response = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    print(content, end="", flush=True)
                    full_response += content
            
            print(f"DEBUG_RECALL: LLM full_response: '{full_response}'")
            return full_response
        
        except Exception as e:
            print(f"\n[API ERROR] {str(e)}")
            traceback.print_exc()
            return "ËØ∑Ê±ÇÂ§ÑÁêÜÈÅáÂà∞ÊäÄÊúØÈóÆÈ¢òÔºåËØ∑Á®çÂêéÂÜçËØï"

    async def dialog_round(self, user_input: str) -> str:
        """Â§ÑÁêÜÂçïËΩÆÂØπËØùÁöÑÂÖ®ÊµÅÁ®ã"""
        self.dialog_buffer.add_thought_step("ÂºÄÂßãËß£ÊûêÁî®Êà∑ÈóÆÈ¢òËØ≠‰πâÊ°ÜÊû∂")
        response = await self.generate_response(user_input)
        self.dialog_buffer.add_dialog(user_input, response)
        return response
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñËÆ∞ÂøÜÁ≥ªÁªüÁªüËÆ°‰ø°ÊÅØ (now reflects core KB via IntegratedSystem)"""
        if hasattr(self.integrated_system, 'knowledge_base_vectors'):
            return {
                "core_knowledge_base_vector_count": len(self.integrated_system.knowledge_base_vectors),
                "last_pda_prediction_error": self.current_prediction_error
            }
        return {
            "core_knowledge_base_vector_count": 0,
            "last_pda_prediction_error": self.current_prediction_error,
            "info": "IntegratedSystem reference not available or KB not found."
        }

async def main():
    # This main is for standalone PDA testing, ensure IntegratedSystem is mocked or available if run this way
    # For full system, main.py in the root directory should be used.
    print("Warning: This is a standalone PDA main. For full system, run main.py from the project root.")
    
    # Mock IntegratedSystem for standalone testing if necessary
    class MockIntegratedSystem:
        async def get_tn_query_vector(self, text):
            print(f"MockIntegratedSystem.get_tn_query_vector called with: {text}")
            # Return a dummy tensor of shape (384,) for testing
            return torch.randn(384)

        def query_core_knowledge_base(self, query_vector, top_k=3):
            print(f"MockIntegratedSystem.query_core_knowledge_base called with vector shape: {query_vector.shape}, top_k: {top_k}")
            # Return dummy retrieved items
            return [
                ('["mock_S", "mock_P", "mock_O"]', torch.randn(384), 0.987),
                ('["another_S", "another_P", "another_O"]', torch.randn(384), 0.876)
            ]

        class LLMConfig: # Mocking inner class
            def __init__(self):
                self.api_key = "test_key" # Actual key not used by mock

        class Config: # Mocking outer class
             def __init__(self):
                self.llm_config = MockIntegratedSystem.LLMConfig()


        knowledge_base_vectors = {"test_key": torch.randn(384)} # Mock KB
        config = Config() # Mock config for PDA init

    # Use actual API key from env or a placeholder if not critical for this specific test
    api_key_to_use = os.getenv("DEEPSEEK_API_KEY", "YOUR_DEEPSEEK_API_KEY_HERE")
    if api_key_to_use == "YOUR_DEEPSEEK_API_KEY_HERE":
        print("Warning: DEEPSEEK_API_KEY not found in environment. LLM calls might fail if not using a mock.")

    print("PDA.py main() is primarily for isolated testing and is commented out to prevent accidental runs without proper IntegratedSystem.")
    print("Please run the main.py from the project root for full system diagnostics.")

if __name__ == "__main__":
    # asyncio.run(main()) # Commented out to prevent accidental direct run
    pass
