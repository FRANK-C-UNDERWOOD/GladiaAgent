"""
ğŸ¯ PredictiveDialogAgent - æ™ºèƒ½å¯¹è¯ä»£ç†æ ¸å¿ƒç³»ç»Ÿ
æŠ€æœ¯æ¶æ„ï¼šé¢„æµ‹ç¼–ç (Predictive Coding) + æ·±åº¦è®°å¿†ç®¡ç† + DeepSeekå¤§æ¨¡å‹é›†æˆ
åŠŸèƒ½äº®ç‚¹ï¼š
1. è‡ªé€‚åº”è®°å¿†è§¦å‘æœºåˆ¶
2. æ€ç»´é“¾(CoT)å¯è§†åŒ–ç®¡ç†
3. åŠ¨æ€ä¸‰å…ƒç»„æŠ½å–
4. è®°å¿†æŒä¹…åŒ–å­˜å‚¨
5. å®æ—¶é¢„æµ‹è¯¯å·®ç›‘æ§
"""
import os
import torch

# è®¾ç½®é•œåƒæºï¼ˆæ”¯æŒHTTPåè®®é¿å…SSLé—®é¢˜ï¼‰
os.environ["HF_ENDPOINT"] = "http://hf-mirror.com"  # å›½å†…é•œåƒæº[5](@ref)

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

from openai import AsyncOpenAI
from collections import deque
from typing import List, Tuple, Dict, Union, Any
import asyncio
import json
import hashlib

# --------------- åŸºç¡€è®°å¿†ç»„ä»¶ (ä¼ªä»£ç ç¤ºæ„ï¼Œå®é™…éœ€å®Œæ•´å®ç°) ---------------
class MemoryAnchorUpdater:
    def __init__(self):
        self.shared_anchors = {}
    
    def update_anchor(self, key: str, weight: float):
        self.shared_anchors[key] = weight
    
    def get_shared_anchors(self) -> Dict[str, float]:
        return self.shared_anchors.copy()

class GraphMemoryNode:
    __slots__ = ('id', 'embedding', 'content', 'last_accessed')
    def __init__(self, id: str, emb: torch.Tensor, content: str):
        self.id = id
        self.embedding = emb
        self.content = content
        self.last_accessed = torch.tensor(0.0)

class GraphMemoryBank:
    def __init__(self):
        self.graph_nodes: Dict[str, GraphMemoryNode] = {}
        self.graph_edges: Dict[str, List[Tuple[str, str]]] = {}
    
    def add_node(self, node: GraphMemoryNode):
        self.graph_nodes[node.id] = node
    
    def add_edge(self, from_id: str, to_id: str, rel: str):
        if from_id not in self.graph_edges:
            self.graph_edges[from_id] = []
        self.graph_edges[from_id].append((to_id, rel))
    
    def save_all(self, path_prefix: str):
        """ä¿å­˜è®°å¿†å›¾åˆ°JSONæ–‡ä»¶ã€‚"""
        nodes_file = f"{path_prefix}_nodes.json"
        edges_file = f"{path_prefix}_edges.json"

        serializable_nodes = []
        for node_id, node in self.graph_nodes.items():
            serializable_nodes.append({
                'id': node.id,
                'embedding': node.embedding.cpu().tolist(), # Ensure tensor is on CPU and convert to list
                'content': node.content,
                'last_accessed': node.last_accessed.cpu().item() # Ensure tensor is on CPU and get scalar value
            })
        
        try:
            with open(nodes_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_nodes, f, indent=4, ensure_ascii=False)
            print(f"ğŸ§  Nodes saved to {nodes_file}")
        except Exception as e:
            print(f"Error saving nodes: {e}")

        try:
            with open(edges_file, 'w', encoding='utf-8') as f:
                json.dump(self.graph_edges, f, indent=4, ensure_ascii=False)
            print(f"ğŸ”— Edges saved to {edges_file}")
        except Exception as e:
            print(f"Error saving edges: {e}")
    
    def load(self, path_prefix: str):
        """ä»JSONæ–‡ä»¶åŠ è½½è®°å¿†å›¾ã€‚"""
        nodes_file = f"{path_prefix}_nodes.json"
        edges_file = f"{path_prefix}_edges.json"

        if os.path.exists(nodes_file):
            try:
                with open(nodes_file, 'r', encoding='utf-8') as f:
                    loaded_nodes_data = json.load(f)
                
                self.graph_nodes.clear()
                for node_data in loaded_nodes_data:
                    node = GraphMemoryNode(
                        id=node_data['id'],
                        emb=torch.tensor(node_data['embedding'], dtype=torch.float32), # Specify dtype
                        content=node_data['content']
                    )
                    node.last_accessed = torch.tensor(node_data['last_accessed'], dtype=torch.float32) # Specify dtype
                    self.graph_nodes[node.id] = node
                print(f"ğŸ§  Nodes loaded from {nodes_file}: {len(self.graph_nodes)} nodes")
            except Exception as e:
                print(f"Error loading nodes: {e}")
        else:
            print(f"Node file {nodes_file} not found. Starting with an empty node bank.")

        if os.path.exists(edges_file):
            try:
                with open(edges_file, 'r', encoding='utf-8') as f:
                    self.graph_edges = json.load(f)
                print(f"ğŸ”— Edges loaded from {edges_file}: {len(self.graph_edges)} edge groups")
            except Exception as e:
                print(f"Error loading edges: {e}")
                self.graph_edges = {} # Reset if loading failed
        else:
            print(f"Edge file {edges_file} not found. Starting with an empty edge bank.")

# --------------- æ ¸å¿ƒå¯¹è¯ä»£ç†å®ç° ---------------
class DialogHistoryBuffer:
    """å¯¹è¯å†å²ä¸æ€ç»´é“¾ç®¡ç†"""
    def __init__(self, max_len=10):
        self.history = deque(maxlen=max_len)
        self.thought_chain = []

    def add_dialog(self, user_input: str, agent_response: str):
        self.history.append(("user", user_input))
        self.history.append(("assistant", agent_response))

    def add_thought_step(self, thought: str):
        self.thought_chain.append(thought)
        if len(self.thought_chain) > 5:
            self.thought_chain.pop(0)

    def context_text(self) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        return "\n".join([f"{'User' if role=='user' else 'AI'}: {text}" 
                         for role, text in self.history])

    def chain_text(self) -> str:
        """æ ¼å¼åŒ–æ€ç»´é“¾"""
        return "\n".join([f"Step {i+1}: {thought}" 
                         for i, thought in enumerate(self.thought_chain)])

    def clear_chain(self):
        self.thought_chain.clear()

class PredictiveCodingCore:
    """é¢„æµ‹ç¼–ç æ ¸å¿ƒå¤„ç†å™¨"""
    def __init__(self, memory: GraphMemoryBank):
        self.memory = memory
        self.current_prediction_error = 0.0
    
    def predict(self, triplet: tuple) -> float:
        """é¢„æµ‹ä¸‰å…ƒç»„çš„è®¤çŸ¥åŒ¹é…åº¦"""
        # ä¼ªä»£ç ï¼šåŸºäºè®°å¿†å›¾è®¡ç®—é¢„æµ‹è¯¯å·®
        self.current_prediction_error = 0.1  # æ¨¡æ‹Ÿè®¡ç®—å€¼
        return self.current_prediction_error
    
    def encode_input(self, text: str) -> torch.Tensor:
        """ç¼–ç è¾“å…¥ä¿¡æ¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        return torch.randn(1, 64)
    
    def update_memory(self, triplet: tuple, embedding: torch.Tensor):
        """æ›´æ–°è®°å¿†ç³»ç»Ÿ"""
        # ç”Ÿæˆå”¯ä¸€è®°å¿†ID
        mem_id = hashlib.md5(str(triplet).encode()).hexdigest()
        
        # åˆ›å»ºè®°å¿†èŠ‚ç‚¹
        node = GraphMemoryNode(
            id=mem_id,
            emb=embedding,
            content=f"{triplet[0]} {triplet[1]} {triplet[2]}"
        )
        self.memory.add_node(node)

class MemoryRetriever:
    """è®°å¿†æ£€ç´¢ç³»ç»Ÿ"""
    def __init__(self, memory_bank: GraphMemoryBank):
        self.memory = memory_bank
    
    def query_memory(self, query_text: str, top_k=3) -> List[str]:
        """æŸ¥è¯¢ç›¸å…³è®°å¿†ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…åº”å®ç°åŸºäºå‘é‡çš„ç›¸ä¼¼åº¦æœç´¢
        return [node.content for node in list(self.memory.graph_nodes.values())[:top_k]]

class PredictiveDialogAgent:
    """é¢„æµ‹ç¼–ç å¯¹è¯ä»£ç†ä¸»ç±»"""
    def __init__(self, 
                 deepseek_api_key: str, 
                 tool_threshold: float = 0.25,
                 memory_threshold: float = 0.15):
        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')
        self.client = AsyncOpenAI(
            base_url="https://api.deepseek.com/v1",
            api_key=deepseek_api_key
        )
        
        # è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–
        self.memory_bank = GraphMemoryBank()
        self.DSAP = MemoryAnchorUpdater()
        self.MPR = MemoryRetriever(self.memory_bank)
        self.pc_core = PredictiveCodingCore(self.memory_bank)
        
        # å¯¹è¯ç®¡ç†ç³»ç»Ÿ
        self.dialog_buffer = DialogHistoryBuffer(max_len=8)
        
        # é˜ˆå€¼è®¾ç½®
        self.tool_trigger_threshold = tool_threshold
        self.memory_trigger_threshold = memory_threshold
        
        # è®°å¿†é”šç‚¹ï¼ˆç¤ºä¾‹ä¸ªæ€§åŒ–è®¾ç½®ï¼‰
        self._init_identity_anchors()
    
    def _init_identity_anchors(self):
        """åˆå§‹åŒ–èº«ä»½é”šç‚¹ï¼ˆé¢†åŸŸçŸ¥è¯†/ä¸ªæ€§ç‰¹å¾ï¼‰"""
        self.DSAP.update_anchor("èº«ä»½::å®éªŒå®¤åŠ©æ‰‹::æ­Œè•¾è’‚å¨…", 1.0)
        self.DSAP.update_anchor("è¯­è¨€é£æ ¼::å­¦æœ¯ä¸¥è°¨æ€§::ä¿æŒä¸“ä¸šæœ¯è¯­", 0.9)
        self.DSAP.update_anchor("å¯¹è¯åå¥½::è¯¦ç»†è§£é‡Š::æä¾›é¢å¤–èƒŒæ™¯çŸ¥è¯†", 0.85)
    
    async def extract_triplet(self, text: str) -> Union[Tuple[str, str, str], None]:
        """ä½¿ç”¨å¤§æ¨¡å‹æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„"""
        PROMPT = (
            "è¯·ä»ä¸‹é¢çš„å¯¹è¯å†…å®¹ä¸­æŠ½å–æ ¸å¿ƒçŸ¥è¯†ä¸‰å…ƒç»„(ä¸»ä½“, å…³ç³», å®¢ä½“)ã€‚"
            "æ ¼å¼è¦æ±‚ï¼šä¸¥æ ¼æŒ‰[ä¸»ä½“, å…³ç³», å®¢ä½“]è¿”å›ï¼Œä¸è¦è§£é‡Šã€‚"
            f"å¯¹è¯å†…å®¹ï¼š{text}"
        )
        
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æœ‰ä¸‰é‡è’¸é¦èƒ½åŠ›çš„çŸ¥è¯†æŠ½å–ä¸“å®¶"},
                    {"role": "user", "content": PROMPT}
                ]
            )
            result = response.choices[0].message.content.strip()
            
            # è§£æä¸‰å…ƒç»„æ ¼å¼
            if result.startswith("[") and result.endswith("]"):
                items = result[1:-1].split(",")
                if len(items) == 3:
                    return tuple(i.strip() for i in items)
        except Exception as e:
            print(f"[ERROR] Triplet extraction failed: {str(e)}")
        return None

    async def generate_response(self, prompt: str) -> str:
        """ç”Ÿæˆå¯¹è¯å“åº”çš„æ ¸å¿ƒé€»è¾‘"""
        # 1. ä¸Šä¸‹æ–‡æ„å»º
        dialog_ctx = self.dialog_buffer.context_text()
        thought_chain = self.dialog_buffer.chain_text() or "é¦–æ¬¡æ€è€ƒè·¯å¾„"
        
        # 2. è®°å¿†æ£€ç´¢ä¸é¢„æµ‹ç¼–ç 
        memory_results = self.MPR.query_memory(prompt)
        memory_block = "\n".join([f"- {mem}" for mem in memory_results]) or "æ— ç›¸å…³è®°å¿†"
        prediction_error = self.pc_core.predict(("ç”¨æˆ·è¾“å…¥", "å½“å‰è¯­ä¹‰", prompt))
        
        # 3. æ„å»ºç³»ç»Ÿæç¤ºï¼ˆå¤šé‡è®¤çŸ¥æ¥æºæ•´åˆï¼‰
        SYSTEM_PROMPT = f"""
# è§’è‰²è®¾å®š
ä½ æ˜¯æ­Œè•¾è’‚å¨…,å®éªŒå®¤çš„ç§‘ç ”åŠ©æ‰‹ï¼Œæ‹¥æœ‰è·¨å­¦ç§‘çŸ¥è¯†å’Œå®éªŒæ•°æ®åˆ†æèƒ½åŠ›ã€‚

# å¯¹è¯å†å²
{dialog_ctx}

# æ€ç»´é“¾è®°å½•
{thought_chain}

# å…³è”è®°å¿†
{memory_block}

# è®¤çŸ¥çŠ¶æ€
é¢„æµ‹è¯¯å·®: {prediction_error:.3f} | {'éœ€è¦æ·±åº¦æ€è€ƒ' if prediction_error > 0.2 else 'å¸¸è§„åº”ç­”æ¨¡å¼'}

# å“åº”è¦æ±‚
1. ä¸¥æ ¼ä¿æŒä¸“ä¸šä¸¥è°¨çš„å­¦æœ¯ç”¨è¯­ä¹ æƒ¯
2. å¯¹å¤æ‚æ¦‚å¿µæä¾›ç®€æ˜èƒŒæ™¯è§£é‡Š
3. æ ¸å¿ƒå›ç­”æ§åˆ¶åœ¨200å­—å†…
4. ç»“å°¾æ ‡æ³¨[æ€è€ƒå®Œæˆ]
"""
        # 4. è°ƒç”¨DeepSeek APIç”Ÿæˆå“åº”
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.35,
                max_tokens=512,
                stream=True
            )
            
            # æµå¼æ¥æ”¶å¤„ç†
            full_response = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    print(content, end="", flush=True)
                    full_response += content
            
            return full_response
        
        except Exception as e:
            print(f"\n[API ERROR] {str(e)}")
            return "è¯·æ±‚å¤„ç†é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•"

    async def dialog_round(self, user_input: str) -> str:
        """å¤„ç†å•è½®å¯¹è¯çš„å…¨æµç¨‹"""
        # Step 1: åŸºç¡€å“åº”ç”Ÿæˆ
        self.dialog_buffer.add_thought_step("å¼€å§‹è§£æç”¨æˆ·é—®é¢˜è¯­ä¹‰æ¡†æ¶")
        response = await self.generate_response(user_input)
        
        # Step 2: é¢„æµ‹ç¼–ç è®°å¿†å†³ç­–
        if self.pc_core.current_prediction_error > self.memory_trigger_threshold:
            self.dialog_buffer.add_thought_step("æ£€æµ‹åˆ°é«˜è®¤çŸ¥è¯¯å·®ï¼Œå¯åŠ¨è®°å¿†æ›´æ–°åè®®")
            triplet = await self.extract_triplet(user_input)
            
            if triplet:
                # åˆ›å»ºè®°å¿†èŠ‚ç‚¹
                emb = self.embedder.encode(user_input)
                self.pc_core.update_memory(triplet, emb)
                
                # æ·»åŠ åˆ°æ€ç»´é“¾
                self.dialog_buffer.add_thought_step(
                    f"æ–°å¢çŸ¥è¯†èŠ‚ç‚¹: {triplet[0]}â†’{triplet[1]}â†’{triplet[2]}"
                )
        
        # Step 3: æ›´æ–°å¯¹è¯å†å²
        self.dialog_buffer.add_dialog(user_input, response)
        return response
    
    def save_memory(self, path: str = "agent_memory.gmb"):
        """æŒä¹…åŒ–è®°å¿†ç³»ç»Ÿ"""
        self.memory_bank.save_all(path)
        print(f"ğŸ’¾ è®°å¿†ç³»ç»Ÿå·²ä¿å­˜åˆ° {path}")
    
    def load_memory(self, path: str = "agent_memory.gmb"):
        """åŠ è½½è®°å¿†ç³»ç»Ÿ"""
        if os.path.exists(path):
            self.memory_bank.load(path)
            print(f"ğŸ” å·²åŠ è½½{len(self.memory_bank.graph_nodes)}ä¸ªè®°å¿†èŠ‚ç‚¹")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_nodes": len(self.memory_bank.graph_nodes),
            "total_edges": sum(len(v) for v in self.memory_bank.graph_edges.values()),
            "last_prediction_error": self.pc_core.current_prediction_error
        }
# ==================== å¯åŠ¨å…¥å£ ====================
async def main():
    # åˆå§‹åŒ–ä»£ç† (éœ€è®¾ç½®çœŸå®APIå¯†é’¥)
    agent = PredictiveDialogAgent(deepseek_api_key="")
    print("PDA é¢„æµ‹å¯¹è¯ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")
    # åŠ è½½å†å²è®°å¿†
    agent.load_memory()
    print("å†å²è®°å¿†åŠ è½½æˆåŠŸ")
    # å¯¹è¯æ¼”ç¤º
    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ")
            if user_input.lower() in ["exit", "quit"]:
                break
                
            print("\nğŸ¤– AI: ", end="")
            response = await agent.dialog_round(user_input)
            
        except KeyboardInterrupt:
            print("\nå¯¹è¯ç»“æŸ")
            break
    
    # ä¿å­˜è®°å¿†
    agent.save_memory()

if __name__ == "__main__":
    asyncio.run(main())
