"""
ğŸ¯ PredictiveDialogAgent - æ™ºèƒ½å¯¹è¯ä»£ç†æ ¸å¿ƒç³»ç»Ÿ
æŠ€æœ¯æ¶æ„ï¼šé¢„æµ‹ç¼–ç (Predictive Coding) + æ·±åº¦è®°å¿†ç®¡ç† + DeepSeekå¤§æ¨¡å‹é›†æˆ
åŠŸèƒ½äº®ç‚¹ï¼š
1. è‡ªé€‚åº”è®°å¿†è§¦å‘æœºåˆ¶
2. æ€ç»´é“¾(CoT)å¯è§†åŒ–ç®¡ç†
3. åŠ¨æ€ä¸‰å…ƒç»„æŠ½å–
4. è®°å¿†æŒä¹…åŒ–å­˜å‚¨
5. å®æ—¶é¢„æµ‹è¯¯å·®ç›‘æ§
"""
import os
import torch

# è®¾ç½®é•œåƒæºï¼ˆæ”¯æŒHTTPåè®®é¿å…SSLé—®é¢˜ï¼‰
os.environ["HF_ENDPOINT"] = "http://hf-mirror.com"  # å›½å†…é•œåƒæº[5](@ref)

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

from openai import AsyncOpenAI
from collections import deque
from typing import List, Tuple, Dict, Union, Any
import asyncio
import json
import hashlib

# --------------- æ ¸å¿ƒå¯¹è¯ä»£ç†å®ç° ---------------
class DialogHistoryBuffer:
    """å¯¹è¯å†å²ä¸æ€ç»´é“¾ç®¡ç†"""
    def __init__(self, max_len=10):
        self.history = deque(maxlen=max_len)
        self.thought_chain = []

    def add_dialog(self, user_input: str, agent_response: str):
        self.history.append(("user", user_input))
        self.history.append(("assistant", agent_response))

    def add_thought_step(self, thought: str):
        self.thought_chain.append(thought)
        if len(self.thought_chain) > 5:
            self.thought_chain.pop(0)

    def context_text(self) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        return "\n".join([f"{'User' if role=='user' else 'AI'}: {text}" 
                         for role, text in self.history])

    def chain_text(self) -> str:
        """æ ¼å¼åŒ–æ€ç»´é“¾"""
        return "\n".join([f"Step {i+1}: {thought}" 
                         for i, thought in enumerate(self.thought_chain)])

    def clear_chain(self):
        self.thought_chain.clear()

class PredictiveDialogAgent:
    """
    é¢„æµ‹ç¼–ç å¯¹è¯ä»£ç†ä¸»ç±»
    Refactored to use a unified memory system managed by IntegratedSystem.
    """
    def __init__(self, 
                 deepseek_api_key: str,
                 integrated_system_ref: Any, # Reference to the IntegratedSystem instance
                 tool_threshold: float = 0.25,
                 memory_threshold: float = 0.15): # memory_threshold might be re-evaluated
        
        self.integrated_system = integrated_system_ref # Store reference to IntegratedSystem

        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2') # May still be used for some local text processing
        self.client = AsyncOpenAI(
            base_url="https://api.deepseek.com/v1",
            api_key=deepseek_api_key
        )
        
       
        self.current_prediction_error = 0.1 # Default placeholder value

        # å¯¹è¯ç®¡ç†ç³»ç»Ÿ
        self.dialog_buffer = DialogHistoryBuffer(max_len=8)
        
        # é˜ˆå€¼è®¾ç½®
        self.tool_trigger_threshold = tool_threshold
        # memory_threshold will need to be re-evaluated in context of new predictive coding logic
        self.memory_trigger_threshold = memory_threshold 
        
       
    
    async def extract_triplet(self, text: str) -> Union[Tuple[str, str, str], None]:
        """ä½¿ç”¨å¤§æ¨¡å‹æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„"""
        PROMPT = (
            "è¯·ä»ä¸‹é¢çš„å¯¹è¯å†…å®¹ä¸­æŠ½å–æ ¸å¿ƒçŸ¥è¯†ä¸‰å…ƒç»„(ä¸»ä½“, å…³ç³», å®¢ä½“)ã€‚"
            "æ ¼å¼è¦æ±‚ï¼šä¸¥æ ¼æŒ‰[ä¸»ä½“, å…³ç³», å®¢ä½“]è¿”å›ï¼Œä¸è¦è§£é‡Šã€‚"
            f"å¯¹è¯å†…å®¹ï¼š{text}"
        )
        
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æœ‰ä¸‰é‡è’¸é¦èƒ½åŠ›çš„çŸ¥è¯†æŠ½å–ä¸“å®¶"},
                    {"role": "user", "content": PROMPT}
                ]
            )
            result = response.choices[0].message.content.strip()
            
            # è§£æä¸‰å…ƒç»„æ ¼å¼
            if result.startswith("[") and result.endswith("]"):
                items = result[1:-1].split(",")
                if len(items) == 3:
                    return tuple(i.strip() for i in items)
        except Exception as e:
            print(f"[ERROR] Triplet extraction failed: {str(e)}")
        return None

    async def generate_response(self, prompt: str) -> str:
        """ç”Ÿæˆå¯¹è¯å“åº”çš„æ ¸å¿ƒé€»è¾‘"""
        # 1. ä¸Šä¸‹æ–‡æ„å»º
        dialog_ctx = self.dialog_buffer.context_text()
        thought_chain = self.dialog_buffer.chain_text() or "é¦–æ¬¡æ€è€ƒè·¯å¾„"
        
        # 2. è®°å¿†æ£€ç´¢ (using Core Unified Memory via IntegratedSystem)
        memory_block = "æ— ç›¸å…³æ ¸å¿ƒçŸ¥è¯†åº“è®°å¿†" # Default if no relevant info found
        query_vector_for_kb = await self.integrated_system.get_tn_query_vector(prompt)

        if query_vector_for_kb is not None:
            # Query the core knowledge base
            # Ensure query_vector_for_kb is on the correct device if IntegratedSystem expects it
            # query_core_knowledge_base expects query_vector to be on CPU for its internal logic
            retrieved_items = self.integrated_system.query_core_knowledge_base(
                query_vector=query_vector_for_kb.cpu(), 
                top_k=3
            )
            if retrieved_items:
                formatted_memory_results = []
                for triple_key_str, _, score in retrieved_items:
                    # triple_key_str is a JSON string of a sorted tuple, e.g., '["å®¢ä½“", "ä¸»ä½“", "å…³ç³»"]'
                    # We can parse it back or use it as is.
                    try:
                        # Attempt to parse the triple_key back into a readable format
                        triple_tuple = tuple(json.loads(triple_key_str))
                        # Reconstruct a more readable string if possible, or use key as is
                        readable_triple = f"({triple_tuple[0]}, {triple_tuple[1]}, {triple_tuple[2]})"
                        formatted_memory_results.append(f"- {readable_triple} (ç›¸ä¼¼åº¦: {score:.3f})")
                    except json.JSONDecodeError:
                        formatted_memory_results.append(f"- {triple_key_str} (ç›¸ä¼¼åº¦: {score:.3f})") # Fallback to raw key

                memory_block = "\n".join(formatted_memory_results)
        else:
            memory_block = "æ— æ³•ä¸ºå½“å‰è¾“å…¥ç”ŸæˆæŸ¥è¯¢å‘é‡"

        # prediction_error is now a member `self.current_prediction_error`
        # It should be updated by IntegratedSystem or a dedicated predictive coding module
        # For now, it uses the placeholder value.
        prediction_error = self.current_prediction_error 
        
        # 3. æ„å»ºç³»ç»Ÿæç¤ºï¼ˆå¤šé‡è®¤çŸ¥æ¥æºæ•´åˆï¼‰
        SYSTEM_PROMPT = f"""
# è§’è‰²è®¾å®š
ä½ æ˜¯æ­Œè•¾è’‚å¨…,å®éªŒå®¤çš„ç§‘ç ”åŠ©æ‰‹ï¼Œæ‹¥æœ‰è·¨å­¦ç§‘çŸ¥è¯†å’Œå®éªŒæ•°æ®åˆ†æèƒ½åŠ›ã€‚

# å¯¹è¯å†å²
{dialog_ctx}

# æ€ç»´é“¾è®°å½•
{thought_chain}

# å…³è”è®°å¿†
{memory_block}

# è®¤çŸ¥çŠ¶æ€
é¢„æµ‹è¯¯å·®: {prediction_error:.3f} | {'éœ€è¦æ·±åº¦æ€è€ƒ' if prediction_error > 0.2 else 'å¸¸è§„åº”ç­”æ¨¡å¼'}

# å“åº”è¦æ±‚
1. ä¸¥æ ¼ä¿æŒä¸“ä¸šä¸¥è°¨çš„å­¦æœ¯ç”¨è¯­ä¹ æƒ¯
2. å¯¹å¤æ‚æ¦‚å¿µæä¾›ç®€æ˜èƒŒæ™¯è§£é‡Š
3. æ ¸å¿ƒå›ç­”æ§åˆ¶åœ¨200å­—å†…
4. ç»“å°¾æ ‡æ³¨[æ€è€ƒå®Œæˆ]
"""
        # 4. è°ƒç”¨DeepSeek APIç”Ÿæˆå“åº”
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.35,
                max_tokens=512,
                stream=True
            )
            
            # æµå¼æ¥æ”¶å¤„ç†
            full_response = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    print(content, end="", flush=True)
                    full_response += content
            
            return full_response
        
        except Exception as e:
            print(f"\n[API ERROR] {str(e)}")
            return "è¯·æ±‚å¤„ç†é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•"

    async def dialog_round(self, user_input: str) -> str:
        """å¤„ç†å•è½®å¯¹è¯çš„å…¨æµç¨‹"""
        # Step 1: åŸºç¡€å“åº”ç”Ÿæˆ
        self.dialog_buffer.add_thought_step("å¼€å§‹è§£æç”¨æˆ·é—®é¢˜è¯­ä¹‰æ¡†æ¶")
        # Note: The process of extracting triples from user_input and updating the 
        # core knowledge base (self.integrated_system.knowledge_base_vectors)
        # is now handled by IntegratedSystem.chat_with_agent -> IntegratedSystem.process()
        # *before* this dialog_round method is called by chat_with_agent.

        response = await self.generate_response(user_input) # generate_response will be updated later to use new KB
        
        
        # Step 3: æ›´æ–°å¯¹è¯å†å²
        self.dialog_buffer.add_dialog(user_input, response)
        return response
    
    # def save_memory(self, path: str = "agent_memory.gmb"): ... (Removed - unified memory saved by IntegratedSystem)
    
    # def load_memory(self, path: str = "agent_memory.gmb"): ... (Removed - unified memory loaded by IntegratedSystem)
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ (now reflects core KB via IntegratedSystem)"""
        if hasattr(self.integrated_system, 'knowledge_base_vectors'):
            # TODO: Add more stats from integrated_system if needed (e.g. TN/SeRNN model info)
            return {
                "core_knowledge_base_vector_count": len(self.integrated_system.knowledge_base_vectors),
                "last_pda_prediction_error": self.current_prediction_error # PDA's own error metric
            }
        return {
            "core_knowledge_base_vector_count": 0,
            "last_pda_prediction_error": self.current_prediction_error,
            "info": "IntegratedSystem reference not available or KB not found."
        }

# ==================== å¯åŠ¨å…¥å£ ====================
async def main():
    # åˆå§‹åŒ–ä»£ç† (éœ€è®¾ç½®çœŸå®APIå¯†é’¥)
    agent = PredictiveDialogAgent(deepseek_api_key="")
    print("PDA é¢„æµ‹å¯¹è¯ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")
    # åŠ è½½å†å²è®°å¿†
    agent.load_memory()
    print("å†å²è®°å¿†åŠ è½½æˆåŠŸ")
    # å¯¹è¯æ¼”ç¤º
    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ")
            if user_input.lower() in ["exit", "quit"]:
                break
                
            print("\nğŸ¤– AI: ", end="")
            response = await agent.dialog_round(user_input)
            
        except KeyboardInterrupt:
            print("\nå¯¹è¯ç»“æŸ")
            break
    
    # ä¿å­˜è®°å¿†
    agent.save_memory()

if __name__ == "__main__":
    asyncio.run(main())
