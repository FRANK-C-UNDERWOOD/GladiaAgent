"""
ğŸ¯ğŸ¯ PredictiveDialogAgent - æ™ºèƒ½å¯¹è¯ä»£ç†æ ¸å¿ƒç³»ç»Ÿ
æŠ€æœ¯æ¶æ„ï¼šé¢„æµ‹ç¼–ç (Predictive Coding) + æ·±åº¦è®°å¿†ç®¡ç† + DeepSeekå¤§æ¨¡å‹é›†æˆ
åŠŸèƒ½äº®ç‚¹ï¼š
1. è‡ªé€‚åº”è®°å¿†è§¦å‘æœºåˆ¶
2. æ€ç»´é“¾(CoT)å¯è§†åŒ–ç®¡ç†
3. åŠ¨æ€ä¸‰å…ƒç»„æŠ½å–
4. è®°å¿†æŒä¹…åŒ–å­˜å‚¨
5. å®æ—¶é¢„æµ‹è¯¯å·®ç›‘æ§
"""
import os
import torch
from sentence_transformers import SentenceTransformer
from openai import AsyncOpenAI # Ensure this is the correct import for your version
from collections import deque
from typing import List, Tuple, Dict, Union, Any, AsyncGenerator
import asyncio
import json
import hashlib
import traceback

# --------------- æ ¸å¿ƒå¯¹è¯ä»£ç†å®ç° ---------------
class DialogHistoryBuffer:
    """å¯¹è¯å†å²ä¸æ€ç»´é“¾ç®¡ç†"""
    def __init__(self, max_len=10):
        self.history = deque(maxlen=max_len)
        self.thought_chain = []

    def add_dialog(self, user_input: str, agent_response: str):
        self.history.append(("user", user_input))
        self.history.append(("assistant", agent_response))

    def add_thought_step(self, thought: str):
        self.thought_chain.append(thought)
        if len(self.thought_chain) > 5:
            self.thought_chain.pop(0)

    def context_text(self) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        return "\n".join([f"{'User' if role=='user' else 'AI'}: {text}" 
                         for role, text in self.history])

    def chain_text(self) -> str:
        """æ ¼å¼åŒ–æ€ç»´é“¾"""
        return "\n".join([f"Step {i+1}: {thought}" 
                         for i, thought in enumerate(self.thought_chain)])

    def clear_chain(self):
        self.thought_chain.clear()

class PredictiveDialogAgent:
    """
    é¢„æµ‹ç¼–ç å¯¹è¯ä»£ç†ä¸»ç±»
    """
    def __init__(self, 
                 deepseek_api_key: str,
                 integrated_system_ref: Any, # Reference to the IntegratedSystem instance
                 tool_threshold: float = 0.25,
                 memory_threshold: float = 0.15): 
        
        self.integrated_system = integrated_system_ref
        self.current_prediction_error = 0.1 # Default placeholder value

        # å¯¹è¯ç®¡ç†ç³»ç»Ÿ
        self.dialog_buffer = DialogHistoryBuffer(max_len=8)
        
        # é˜ˆå€¼è®¾ç½®
        self.tool_trigger_threshold = tool_threshold
        self.memory_trigger_threshold = memory_threshold 
        
        # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            base_url="https://api.deepseek.com/v1",
            api_key=deepseek_api_key
        )
    
    async def extract_triplet(self, text: str) -> Union[Tuple[str, str, str], None]:
        """ä½¿ç”¨å¤§æ¨¡å‹æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„"""
        PROMPT = (
            "è¯·ä»ä¸‹é¢çš„å¯¹è¯å†…å®¹ä¸­æŠ½å–æ ¸å¿ƒçŸ¥è¯†ä¸‰å…ƒç»„(ä¸»ä½“, å…³ç³», å®¢ä½“)ã€‚"
            "æ ¼å¼è¦æ±‚ï¼šä¸¥æ ¼æŒ‰[ä¸»ä½“, å…³ç³», å®¢ä½“]è¿”å›ï¼Œä¸è¦è§£é‡Šã€‚"
            f"å¯¹è¯å†…å®¹ï¼š{text}"
        )
        
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æœ‰ä¸‰é‡è’¸é¦èƒ½åŠ›çš„çŸ¥è¯†æŠ½å–ä¸“å®¶"},
                    {"role": "user", "content": PROMPT}
                ]
            )
            result = response.choices[0].message.content.strip()
            
            # è§£æä¸‰å…ƒç»„æ ¼å¼
            if result.startswith("[") and result.endswith("]"):
                items = result[1:-1].split(",")
                if len(items) == 3:
                    return tuple(i.strip() for i in items)
        except Exception as e:
            print(f"[ERROR] Triplet extraction failed: {str(e)}")
        return None

    async def generate_response(self, prompt: str) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆå¯¹è¯å“åº”çš„æ ¸å¿ƒé€»è¾‘"""
        print(f"DEBUG_RECALL: PDA.generate_response called with prompt: '{prompt}'")
        # 1. ä¸Šä¸‹æ–‡æ„å»º
        dialog_ctx = self.dialog_buffer.context_text()
        thought_chain = self.dialog_buffer.chain_text() or "é¦–æ¬¡æ€è€ƒè·¯å¾„"
        
        # 2. è®°å¿†æ£€ç´¢ (ä½¿ç”¨æ–°çš„åµŒå…¥æ–¹æ³•)
        memory_block = "æ— ç›¸å…³æ ¸å¿ƒçŸ¥è¯†åº“è®°å¿†"
        query_vector_for_kb = None

        print(f"DEBUG_RECALL: Attempting to generate embedding for prompt: '{prompt}'")
        try:
            # ä½¿ç”¨æ–°çš„åµŒå…¥æ–¹æ³•
            query_vector_for_kb = self.integrated_system.generate_embeddings(prompt)
            print(f"DEBUG_RECALL: Successfully generated embedding. Shape: {query_vector_for_kb.shape}")
        except Exception as e_qv:
            print(f"DEBUG_RECALL: Error generating embedding: {e_qv}")
            traceback.print_exc()

        if query_vector_for_kb is not None:
            print(f"DEBUG_RECALL: Querying core knowledge base with generated embedding")
            # Query the core knowledge base
            try:
                retrieved_items = self.integrated_system.query_core_knowledge_base(
                    query_vector=query_vector_for_kb,
                    top_k=3
                )
            except Exception as e_qkb:
                print(f"DEBUG_RECALL: Error querying knowledge base: {e_qkb}")
                traceback.print_exc()
                retrieved_items = []

            print(f"DEBUG_RECALL: Retrieved_items from KB: {len(retrieved_items)} items")
            if retrieved_items:
                formatted_memory_results = []
                for triple_key_str, _, score in retrieved_items:
                    try:
                        triple_tuple = tuple(json.loads(triple_key_str))
                        readable_triple = f"({triple_tuple[0]}, {triple_tuple[1]}, {triple_tuple[2]})"
                        formatted_memory_results.append(f"- {readable_triple} (ç›¸ä¼¼åº¦: {score:.3f})")
                    except json.JSONDecodeError:
                        formatted_memory_results.append(f"- {triple_key_str} (ç›¸ä¼¼åº¦: {score:.3f})")
                if formatted_memory_results:
                    memory_block = "\n".join(formatted_memory_results)
                else:
                    memory_block = "æ ¸å¿ƒçŸ¥è¯†åº“æ£€ç´¢åˆ°å†…å®¹ï¼Œä½†æ ¼å¼åŒ–å¤±è´¥ã€‚"
                print(f"DEBUG_RECALL: Formatted memory_block: {memory_block}")
            else:
                memory_block = "æ ¸å¿ƒçŸ¥è¯†åº“æœªæ£€ç´¢åˆ°ç›¸å…³è®°å¿†ã€‚"
                print(f"DEBUG_RECALL: No items retrieved from core knowledge base.")
        else:
            memory_block = "æ— æ³•ä¸ºå½“å‰è¾“å…¥ç”ŸæˆæŸ¥è¯¢å‘é‡"
            print(f"DEBUG_RECALL: query_vector_for_kb is None. Memory block set to: '{memory_block}'")

        prediction_error = self.current_prediction_error 
        print(f"DEBUG_RECALL: Using prediction_error: {prediction_error:.3f} for LLM prompt.")
        
        # 3. æ„å»ºç³»ç»Ÿæç¤ºï¼ˆå¤šé‡è®¤çŸ¥æ¥æºæ•´åˆï¼‰
        SYSTEM_PROMPT = f"""
# è§’è‰²è®¾å®š
ä½ æ˜¯æ­Œè•¾è’‚å¨…,å®éªŒå®¤çš„ç§‘ç ”åŠ©æ‰‹ï¼Œæ‹¥æœ‰è·¨å­¦ç§‘çŸ¥è¯†å’Œå®éªŒæ•°æ®åˆ†æèƒ½åŠ›ã€‚

# å¯¹è¯å†å²
{dialog_ctx}

# æ€ç»´é“¾è®°å½•
{thought_chain}

# å…³è”è®°å¿†
{memory_block}

# è®¤çŸ¥çŠ¶æ€
é¢„æµ‹è¯¯å·®: {prediction_error:.3f} | {'éœ€è¦æ·±åº¦æ€è€ƒ' if prediction_error > 0.2 else 'å¸¸è§„åº”ç­”æ¨¡å¼'}

# å“åº”è¦æ±‚
1. ä¸¥æ ¼ä¿æŒä¸“ä¸šä¸¥è°¨çš„å­¦æœ¯ç”¨è¯­ä¹ æƒ¯
2. å¯¹å¤æ‚æ¦‚å¿µæä¾›ç®€æ˜èƒŒæ™¯è§£é‡Š
3. æ ¸å¿ƒå›ç­”æ§åˆ¶åœ¨200å­—å†…
4. ç»“å°¾æ ‡æ³¨[æ€è€ƒå®Œæˆ]
"""
        # 4. è°ƒç”¨DeepSeek APIç”Ÿæˆå“åº”
        try:
            print(f"DEBUG_RECALL: Calling LLM with System Prompt")
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.35,
                max_tokens=512,
                stream=True
            )
            
            # full_response = "" # No longer accumulating here
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    # print(content, end="", flush=True) # Optional: keep for server-side logging
                    yield content # Yield the content chunk
            
            # print(f"DEBUG_RECALL: LLM stream finished.") # Optional
        
        except Exception as e:
            print(f"\n[API ERROR] {str(e)}")
            traceback.print_exc()
            yield "è¯·æ±‚å¤„ç†é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•" # Yield error message as a chunk

    async def dialog_round(self, user_input: str) -> AsyncGenerator[str, None]:
        """å¤„ç†å•è½®å¯¹è¯çš„å…¨æµç¨‹ (now an async generator)"""
        self.dialog_buffer.add_thought_step("å¼€å§‹è§£æç”¨æˆ·é—®é¢˜è¯­ä¹‰æ¡†æ¶")
        
        full_streamed_response = ""
        async for chunk in self.generate_response(user_input):
            full_streamed_response += chunk
            yield chunk
            
        # Add the complete response to dialog buffer after streaming is done
        self.dialog_buffer.add_dialog(user_input, full_streamed_response)
        # No return value needed for async generator in this context of yielding
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if hasattr(self.integrated_system, 'knowledge_base_vectors'):
            return {
                "core_knowledge_base_vector_count": len(self.integrated_system.knowledge_base_vectors),
                "last_pda_prediction_error": self.current_prediction_error
            }
        return {
            "core_knowledge_base_vector_count": 0,
            "last_pda_prediction_error": self.current_prediction_error,
            "info": "IntegratedSystem reference not available or KB not found."
        }
    # åœ¨PredictiveDialogAgentç±»ä¸­ä¿æŒåŸæœ‰æµå¼ç”Ÿæˆå™¨ä¸å˜
    
    async def dialog_round(self, user_input: str) -> AsyncGenerator[str, None]:
        """å¤„ç†å•è½®å¯¹è¯çš„å…¨æµç¨‹ (ç°åœ¨æ˜¯ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨)"""
        self.dialog_buffer.add_thought_step("å¼€å§‹è§£æç”¨æˆ·é—®é¢˜è¯­ä¹‰æ¡†æ¶")
        
        full_streamed_response = ""
        async for chunk in self.generate_response(user_input):
            full_streamed_response += chunk
            yield chunk
            
        # å°†å®Œæ•´å“åº”æ·»åŠ åˆ°å¯¹è¯ç¼“å†²åŒº
        self.dialog_buffer.add_dialog(user_input, full_streamed_response)
# æµ‹è¯•ä¸»å‡½æ•°
async def main():
    print("PDAæ¨¡å—è‡ªæ£€å®Œæˆã€‚è¯·é€šè¿‡ä¸»ç³»ç»Ÿæ¥å£è°ƒç”¨å¯¹è¯åŠŸèƒ½ã€‚")

if __name__ == "__main__":
    pass
