"""
ğŸ§ ğŸ§  Tensor Network Compression Module for Triple Embeddings
Inspired by the paper: "Compressing Neural Networks Using Tensor Networks with Exponentially Fewer Variational Parameters"
Author: DOCTOR + æ­Œè•¾è’‚å¨… (2025)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class TripleCompressor(nn.Module):
    """ä¸‰å…ƒç»„å‹ç¼©å™¨ - ç»Ÿä¸€ä½¿ç”¨384ç»´åµŒå…¥"""
    
    def __init__(self, embed_dim=384, num_entities=10000, num_relations=100, device='cpu'):
        super(TripleCompressor, self).__init__()
        
        self.embed_dim = embed_dim  # åµŒå…¥ç»´åº¦è®¾ä¸º384
        self.num_entities = num_entities
        self.num_relations = num_relations
        self.device = device
        
        # å®ä½“å’Œå…³ç³»åµŒå…¥å±‚ - éƒ½ä½¿ç”¨384ç»´
        self.entity_embeddings = nn.Embedding(num_entities, embed_dim)
        self.relation_embeddings = nn.Embedding(num_relations, embed_dim)
        
        # åˆå§‹åŒ–åµŒå…¥æƒé‡
        nn.init.xavier_uniform_(self.entity_embeddings.weight)
        nn.init.xavier_uniform_(self.relation_embeddings.weight)
        
        # ä¸‰å…ƒç»„èåˆå±‚
        self.triplet_fusion = nn.Sequential(
            nn.Linear(embed_dim * 3, embed_dim * 2),  # ä¸‰ä¸ª384ç»´å‘é‡èåˆ
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embed_dim * 2, embed_dim),      # å‹ç¼©åˆ°384ç»´
            nn.LayerNorm(embed_dim)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # æœ€ç»ˆå‹ç¼©å±‚
        self.final_compression = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embed_dim, embed_dim)  # ç¡®ä¿è¾“å‡ºä¸º384ç»´
        )
        
        # ç§»åˆ°æŒ‡å®šè®¾å¤‡
        self.to(device)
    
    def forward(self, triplets):
        """
        å‰å‘ä¼ æ’­
        Args:
            triplets: ä¸‰å…ƒç»„å¼ é‡ [batch_size, 3] - [head, relation, tail]
        Returns:
            compressed: å‹ç¼©åçš„å‘é‡ [batch_size, 384]
        """
        batch_size = triplets.size(0)
        
        # åˆ†ç¦»å¤´å®ä½“ã€å…³ç³»ã€å°¾å®ä½“
        heads = triplets[:, 0]      # [batch_size]
        relations = triplets[:, 1]  # [batch_size]
        tails = triplets[:, 2]      # [batch_size]
        
        # è·å–åµŒå…¥å‘é‡ - æ¯ä¸ªéƒ½æ˜¯384ç»´
        head_embeds = self.entity_embeddings(heads)      # [batch_size, 384]
        relation_embeds = self.relation_embeddings(relations)  # [batch_size, 384]
        tail_embeds = self.entity_embeddings(tails)      # [batch_size, 384]
        
        # æ‹¼æ¥ä¸‰å…ƒç»„åµŒå…¥
        triplet_concat = torch.cat([head_embeds, relation_embeds, tail_embeds], dim=1)  # [batch_size, 1152]
        
        # ä¸‰å…ƒç»„èåˆ
        fused = self.triplet_fusion(triplet_concat)  # [batch_size, 384]
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        # é‡å¡‘ä¸ºåºåˆ—å½¢å¼ç”¨äºæ³¨æ„åŠ›è®¡ç®—
        fused_seq = fused.unsqueeze(1)  # [batch_size, 1, 384]
        attended, _ = self.attention(fused_seq, fused_seq, fused_seq)  # [batch_size, 1, 384]
        attended = attended.squeeze(1)  # [batch_size, 384]
        
        # æ®‹å·®è¿æ¥
        attended = attended + fused
        
        # æœ€ç»ˆå‹ç¼©
        compressed = self.final_compression(attended)  # [batch_size, 384]
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        assert compressed.size(1) == self.embed_dim, f"è¾“å‡ºç»´åº¦é”™è¯¯: {compressed.size(1)} != {self.embed_dim}"
        
        return compressed
    
    def compress_triplet(self, triplet_tensor):
        """
        å…¼å®¹æ€§æ–¹æ³• - å‹ç¼©å•ä¸ªä¸‰å…ƒç»„
        Args:
            triplet_tensor: ä¸‰å…ƒç»„å¼ é‡
        Returns:
            compressed: 384ç»´å‹ç¼©å‘é‡
        """
        return self.forward(triplet_tensor)
    
    def get_entity_embedding(self, entity_id):
        """è·å–å®ä½“åµŒå…¥"""
        return self.entity_embeddings(torch.tensor(entity_id, device=self.device))
    
    def get_relation_embedding(self, relation_id):
        """è·å–å…³ç³»åµŒå…¥"""
        return self.relation_embeddings(torch.tensor(relation_id, device=self.device))


class TensorNetworkLayer(nn.Module):
    """å¼ é‡ç½‘ç»œå±‚ - ç”¨äºæ›´å¤æ‚çš„ä¸‰å…ƒç»„å»ºæ¨¡"""
    
    def __init__(self, embed_dim=384):
        super(TensorNetworkLayer, self).__init__()
        self.embed_dim = embed_dim
        
        # å¼ é‡æ ¸å¿ƒ
        self.tensor_core = nn.Parameter(torch.randn(embed_dim, embed_dim, embed_dim))
        
        # çº¿æ€§å˜æ¢å±‚
        self.linear_transform = nn.Linear(embed_dim, embed_dim)
        
        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.tensor_core)
        nn.init.xavier_uniform_(self.linear_transform.weight)
    
    def forward(self, head_embed, relation_embed, tail_embed):
        """
        å¼ é‡ç½‘ç»œå‰å‘ä¼ æ’­
        Args:
            head_embed: å¤´å®ä½“åµŒå…¥ [batch_size, 384]
            relation_embed: å…³ç³»åµŒå…¥ [batch_size, 384]  
            tail_embed: å°¾å®ä½“åµŒå…¥ [batch_size, 384]
        Returns:
            output: å¼ é‡ç½‘ç»œè¾“å‡º [batch_size, 384]
        """
        batch_size = head_embed.size(0)
        
        # å¼ é‡ä¹˜ç§¯è¿ç®—
        # è®¡ç®— head * tensor_core * tail 
        temp = torch.einsum('bi,ijk,bj->bk', head_embed, self.tensor_core, tail_embed)
        
        # ä¸å…³ç³»åµŒå…¥ç»“åˆ
        combined = temp * relation_embed
        
        # çº¿æ€§å˜æ¢
        output = self.linear_transform(combined)
        
        return output


def validate_dimensions():
    """éªŒè¯ç»´åº¦è®¾ç½®æ˜¯å¦æ­£ç¡®"""
    print("å¼€å§‹éªŒè¯TNæ¨¡å—ç»´åº¦è®¾ç½®...")
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    compressor = TripleCompressor(embed_dim=384, num_entities=1000, num_relations=50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    test_triplets = torch.randint(0, 100, (batch_size, 3))
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = compressor(test_triplets)
    
    # éªŒè¯è¾“å‡ºç»´åº¦
    expected_shape = (batch_size, 384)
    actual_shape = output.shape
    
    assert actual_shape == expected_shape, f"ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{expected_shape}, å®é™…{actual_shape}"
    
    print(f"âœ“ ç»´åº¦éªŒè¯é€šè¿‡")
    print(f"  - è¾“å…¥shape: {test_triplets.shape}")
    print(f"  - è¾“å‡ºshape: {output.shape}")
    print(f"  - åµŒå…¥ç»´åº¦: {compressor.embed_dim}")
    
    return True


if __name__ == "__main__":
    # è¿è¡Œç»´åº¦éªŒè¯
    validate_dimensions()
    print("TNæ¨¡å—é…ç½®å®Œæˆï¼Œæ‰€æœ‰ç»´åº¦å·²å¯¹é½è‡³384ç»´")
