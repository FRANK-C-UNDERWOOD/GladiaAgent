"""
ğŸ§  seRNN æ¨¡å—ï¼šSpatially-Embedded Recurrent Neural Network
Author: DOCTOR + æ­Œè•¾è’‚å¨… (2025)

æ¨¡å—ç”¨é€”ï¼š
- åœ¨ RNN ä¸­åŠ å…¥â€œç¥ç»å…ƒç©ºé—´ä½ç½®â€ä½œä¸ºè¿æ¥ç»“æ„é™åˆ¶
- å®ç°ç©ºé—´ç¨€ç–æ€§çº¦æŸï¼Œæ›´ç¬¦åˆç”Ÿç‰©ç¥ç»ç½‘ç»œçš„è¿æ¥æ¨¡å¼
- å¯ç”¨äº Agent ç©ºé—´å¯¼èˆªè®°å¿†ã€è„‘è¿æ¥æ¨¡æ‹Ÿã€å›¾å¼è®°å¿†å»ºæ„

ä¸»è¦ç»„ä»¶ï¼š
1. seRNNCell       - å•ä¸ªæ—¶é—´æ­¥çš„å¸¦ç©ºé—´æƒ©ç½šçš„ RNN å•å…ƒ
2. seRNN           - å¤šæ­¥åºåˆ—å»ºæ¨¡çš„å¾ªç¯ç½‘ç»œç»“æ„
3. spatial_regularizer - è¿æ¥è·ç¦»æ­£åˆ™é¡¹ï¼ˆç”¨äºåŠ æƒ lossï¼‰
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional, List
import math

class PredictiveCodingLayer(nn.Module):
    """é¢„æµ‹ç¼–ç å±‚ - å®ç°é¢„æµ‹è¯¯å·®è®¡ç®—å’Œè‡ªä¸Šè€Œä¸‹çš„é¢„æµ‹"""
    
    def __init__(self, input_size: int, hidden_size: int, prediction_size: int):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.prediction_size = prediction_size
        
        # é¢„æµ‹ç½‘ç»œ (è‡ªä¸Šè€Œä¸‹) - ä¿®å¤ç»´åº¦åŒ¹é…é—®é¢˜
        self.prediction_net = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, prediction_size)
        )
        
        # è¯¯å·®ç½‘ç»œ (è‡ªä¸‹è€Œä¸Š) - ç¡®ä¿è¾“å…¥è¾“å‡ºç»´åº¦æ­£ç¡®
        self.error_net = nn.Sequential(
            nn.Linear(prediction_size, hidden_size // 2),  # å‡å°‘ç»´åº¦é¿å…è¿‡æ‹Ÿåˆ
            nn.ReLU(),
            nn.Linear(hidden_size // 2, hidden_size)
        )
        
        # ä¸ç¡®å®šæ€§ä¼°è®¡
        self.uncertainty_net = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, 1),
            nn.Softplus()
        )
        
        # è¾“å…¥æŠ•å½±å±‚ - å¤„ç†ç»´åº¦ä¸åŒ¹é…
        if input_size != prediction_size:
            self.input_projection = nn.Linear(input_size, prediction_size)
        else:
            self.input_projection = nn.Identity()
        
    def forward(self, input_data: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        # æŠ•å½±è¾“å…¥åˆ°æ­£ç¡®ç»´åº¦
        projected_input = self.input_projection(input_data)
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = self.prediction_net(hidden_state)
        
        # è®¡ç®—é¢„æµ‹è¯¯å·® - ç°åœ¨ç»´åº¦åŒ¹é…
        prediction_error = projected_input - prediction
        
        # å¤„ç†è¯¯å·®ä¿¡å·
        error_signal = self.error_net(prediction_error)
        
        # ä¼°è®¡ä¸ç¡®å®šæ€§
        uncertainty = self.uncertainty_net(hidden_state)
        
        return prediction, error_signal, uncertainty

class SelectiveGatingMechanism(nn.Module):
    """é€‰æ‹©æ€§é—¨æ§æœºåˆ¶ - å¢å¼ºç‰ˆï¼ŒåŒ…å«é¢„æµ‹è¯¯å·®é©±åŠ¨çš„æ³¨æ„åŠ›"""
    
    def __init__(self, input_size: int, hidden_size: int, spatial_dim: Optional[int] = None):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.spatial_dim = spatial_dim
        
        # è¾“å…¥æŠ•å½±å±‚ - ç¡®ä¿ç»´åº¦åŒ¹é…
        self.input_projection = nn.Linear(input_size, hidden_size) if input_size != hidden_size else nn.Identity()
        
        # ä¼ ç»Ÿé—¨æ§ - ä½¿ç”¨æŠ•å½±åçš„ç»´åº¦
        gate_input_size = hidden_size + hidden_size  # æŠ•å½±åçš„è¾“å…¥ + éšè—çŠ¶æ€
        self.forget_gate = nn.Linear(gate_input_size, hidden_size)
        self.input_gate = nn.Linear(gate_input_size, hidden_size)
        self.candidate_gate = nn.Linear(gate_input_size, hidden_size)
        self.output_gate = nn.Linear(gate_input_size, hidden_size)
        
        # é¢„æµ‹è¯¯å·®é©±åŠ¨çš„æ³¨æ„åŠ›é—¨æ§
        self.error_attention = nn.Linear(hidden_size, hidden_size)
        
        # ç©ºé—´æ³¨æ„åŠ› (å¦‚æœæä¾›äº†ç©ºé—´ç»´åº¦)
        if spatial_dim:
            self.spatial_attention = nn.Linear(hidden_size, spatial_dim)
        
        # æ—¶é—´å°ºåº¦è‡ªé€‚åº”
        self.temporal_scaling = nn.Parameter(torch.ones(1))
        
    def forward(self, input_data: torch.Tensor, hidden_state: torch.Tensor, 
                cell_state: torch.Tensor, error_signal: torch.Tensor, 
                uncertainty: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        
        # æŠ•å½±è¾“å…¥åˆ°éšè—ç»´åº¦
        projected_input = self.input_projection(input_data)
        
        # ç»„åˆè¾“å…¥
        combined = torch.cat([projected_input, hidden_state], dim=-1)
        
        # åŸºæœ¬é—¨æ§
        forget = torch.sigmoid(self.forget_gate(combined))
        input_gate = torch.sigmoid(self.input_gate(combined))
        candidate = torch.tanh(self.candidate_gate(combined))
        output = torch.sigmoid(self.output_gate(combined))
        
        # è¯¯å·®é©±åŠ¨çš„æ³¨æ„åŠ›æƒé‡
        error_attention = torch.sigmoid(self.error_attention(error_signal))
        
        # ä¸ç¡®å®šæ€§è°ƒèŠ‚çš„é—¨æ§
        uncertainty_modulated_input = input_gate * (1 + uncertainty.squeeze(-1))
        uncertainty_modulated_forget = forget * (1 - uncertainty.squeeze(-1) * 0.1)
        
        # æ›´æ–°ç»†èƒçŠ¶æ€
        cell_state = (uncertainty_modulated_forget * cell_state + 
                     uncertainty_modulated_input * candidate * error_attention)
        
        # æ—¶é—´å°ºåº¦è‡ªé€‚åº”
        cell_state = cell_state * self.temporal_scaling
        
        # è¾“å‡ºé—¨æ§
        hidden_state = output * torch.tanh(cell_state)
        
        return hidden_state, cell_state

class SpatialEmbeddingLayer(nn.Module):
    """ç©ºé—´åµŒå…¥å±‚ - å¤„ç†ç©ºé—´ç»“æ„ä¿¡æ¯"""
    
    def __init__(self, hidden_size: int, spatial_dim: int, embedding_dim: int):
        super().__init__()
        self.hidden_size = hidden_size
        self.spatial_dim = spatial_dim
        self.embedding_dim = embedding_dim
        
        # ç©ºé—´ä½ç½®ç¼–ç 
        self.spatial_embedding = nn.Embedding(spatial_dim, embedding_dim)
        
        # ç©ºé—´å…³ç³»å»ºæ¨¡ - ç¡®ä¿ç»´åº¦åŒ¹é…
        self.spatial_transform = nn.Linear(embedding_dim, hidden_size)
        
        # è·ç¦»è¡°å‡å‚æ•°
        self.distance_decay = nn.Parameter(torch.tensor(1.0))
        
        # å±‚æ ‡å‡†åŒ–
        self.layer_norm = nn.LayerNorm(hidden_size)
        
    def forward(self, hidden_states: torch.Tensor, spatial_positions: torch.Tensor) -> torch.Tensor:
        """
        Args:
            hidden_states: [batch_size, seq_len, hidden_size]
            spatial_positions: [batch_size, seq_len] - ç©ºé—´ä½ç½®ç´¢å¼•
        """
        # è·å–ç©ºé—´åµŒå…¥
        spatial_emb = self.spatial_embedding(spatial_positions)
        spatial_features = self.spatial_transform(spatial_emb)
        
        # è®¡ç®—ç©ºé—´è·ç¦»æƒé‡
        batch_size, seq_len, _ = hidden_states.shape
        distance_matrix = torch.abs(spatial_positions.unsqueeze(-1) - spatial_positions.unsqueeze(-2)).float()
        distance_weights = torch.exp(-distance_matrix * self.distance_decay)
        
        # åº”ç”¨ç©ºé—´æƒé‡
        weighted_hidden = torch.bmm(distance_weights, hidden_states)
        
        # ç»“åˆç©ºé—´ç‰¹å¾
        enhanced_hidden = hidden_states + spatial_features + weighted_hidden * 0.1
        
        # å±‚æ ‡å‡†åŒ–
        enhanced_hidden = self.layer_norm(enhanced_hidden)
        
        return enhanced_hidden

class SeRNN(nn.Module):
    """å¢å¼ºç‰ˆseRNN - æ•´åˆé¢„æµ‹ç¼–ç æ¡†æ¶ï¼Œæ”¯æŒ384ç»´è¯åµŒå…¥"""
    
    def __init__(self, 
                 input_size: int = 384,  # é»˜è®¤384ç»´è¯åµŒå…¥
                 hidden_size: int = 384,  # ä¿æŒä¸€è‡´çš„éšè—ç»´åº¦
                 spatial_dim: int = 1000,
                 num_layers: int = 3,
                 dropout: float = 0.1,
                 hierarchical_scales: List[int] = [1, 4, 16],
                 prediction_size: Optional[int] = None,
                 device: str = 'cpu'):  # æ–°å¢deviceå‚æ•°
        super().__init__()
        
        self.device = device  # ä¿å­˜è®¾å¤‡ä¿¡æ¯
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.spatial_dim = spatial_dim
        self.num_layers = num_layers
        self.hierarchical_scales = hierarchical_scales
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé¢„æµ‹å¤§å°ï¼Œä½¿ç”¨è¾“å…¥å¤§å°
        self.prediction_size = prediction_size or input_size
        
        # åˆå§‹åŒ–å„ä¸ªå±‚
        self.pc_layers = nn.ModuleList()
        for i in range(num_layers):
            layer_input_size = input_size if i == 0 else hidden_size
            self.pc_layers.append(
                PredictiveCodingLayer(layer_input_size, hidden_size, self.prediction_size)
            )
        
        # é€‰æ‹©æ€§é—¨æ§æœºåˆ¶
        self.selective_gates = nn.ModuleList()
        for i in range(num_layers):
            layer_input_size = input_size if i == 0 else hidden_size
            self.selective_gates.append(
                SelectiveGatingMechanism(layer_input_size, hidden_size, spatial_dim)
            )
        
        # ç©ºé—´åµŒå…¥å±‚
        self.spatial_embedding = SpatialEmbeddingLayer(
            hidden_size, spatial_dim, min(hidden_size // 2, 192)
        )
        
        # å±‚æ¬¡åŒ–é¢„æµ‹ç½‘ç»œ
        self.hierarchical_predictors = nn.ModuleList([nn.LSTM(hidden_size, hidden_size, batch_first=True)
                                                      for _ in hierarchical_scales])
        
        # å¤šå°ºåº¦èåˆ
        self.scale_fusion = nn.Sequential(
            nn.Linear(len(hierarchical_scales) * hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU()
        )
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, self.prediction_size)
        )
        
        self.dropout = nn.Dropout(dropout)
        self.prediction_loss_weight = nn.Parameter(torch.tensor(1.0))
        self.uncertainty_loss_weight = nn.Parameter(torch.tensor(0.1))
        
        self.to(self.device)  # è¿™é‡Œå°†æ¨¡å‹è½¬ç§»åˆ°æŒ‡å®šè®¾å¤‡ä¸Š

        
    def forward(self, 
                input_sequence: torch.Tensor, 
                spatial_positions: torch.Tensor,
                hidden_states: Optional[List[torch.Tensor]] = None,
                cell_states: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, dict]:
        """
        Args:
            input_sequence: [batch_size, seq_len, input_size] - 384ç»´è¯åµŒå…¥
            spatial_positions: [batch_size, seq_len] - ç©ºé—´ä½ç½®
            hidden_states: List of hidden states for each layer
            cell_states: List of cell states for each layer
        """
        batch_size, seq_len, _ = input_sequence.shape
        device = input_sequence.device
        
        # åˆå§‹åŒ–çŠ¶æ€
        if hidden_states is None:
            hidden_states = [torch.zeros(batch_size, self.hidden_size, device=device) 
                           for _ in range(self.num_layers)]
        if cell_states is None:
            cell_states = [torch.zeros(batch_size, self.hidden_size, device=device) 
                          for _ in range(self.num_layers)]
        
        # å­˜å‚¨è¾“å‡ºå’Œä¸­é—´ç»“æœ
        outputs = []
        all_predictions = []
        all_uncertainties = []
        prediction_errors = []
        
        # é€æ—¶é—´æ­¥å¤„ç†
        for t in range(seq_len):
            current_input = input_sequence[:, t, :]
            layer_input = current_input
            
            # é€å±‚å¤„ç†
            for layer_idx in range(self.num_layers):
                # é¢„æµ‹ç¼–ç 
                prediction, error_signal, uncertainty = self.pc_layers[layer_idx](
                    layer_input, hidden_states[layer_idx])
                
                # é€‰æ‹©æ€§é—¨æ§
                hidden_states[layer_idx], cell_states[layer_idx] = self.selective_gates[layer_idx](
                    layer_input, hidden_states[layer_idx], cell_states[layer_idx], 
                    error_signal, uncertainty)
                
                # åº”ç”¨dropout
                hidden_states[layer_idx] = self.dropout(hidden_states[layer_idx])
                
                # ä¸ºä¸‹ä¸€å±‚å‡†å¤‡è¾“å…¥
                layer_input = hidden_states[layer_idx]
                
                # è®°å½•é¢„æµ‹å’Œä¸ç¡®å®šæ€§ï¼ˆåªè®°å½•æœ€åä¸€å±‚ï¼‰
                if layer_idx == self.num_layers - 1:
                    all_predictions.append(prediction)
                    all_uncertainties.append(uncertainty)
                    # è®¡ç®—é¢„æµ‹è¯¯å·® - ç¡®ä¿ç»´åº¦åŒ¹é…
                    if prediction.shape[-1] == current_input.shape[-1]:
                        pred_error = torch.abs(current_input - prediction).mean(dim=-1, keepdim=True)
                    else:
                        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æŠ•å½±åçš„è¯¯å·®
                        pred_error = torch.abs(error_signal).mean(dim=-1, keepdim=True)
                    prediction_errors.append(pred_error)
            
            # å±‚æ¬¡åŒ–é¢„æµ‹
            hierarchical_outputs = []
            for scale_idx, scale in enumerate(self.hierarchical_scales):
                if t % scale == 0:  # æŒ‰ä¸åŒæ—¶é—´å°ºåº¦é‡‡æ ·
                    scale_input = hidden_states[-1].unsqueeze(1)
                    scale_output, _ = self.hierarchical_predictors[scale_idx](scale_input)
                    hierarchical_outputs.append(scale_output.squeeze(1))
                else:
                    hierarchical_outputs.append(torch.zeros_like(hidden_states[-1]))
            
            # å¤šå°ºåº¦èåˆ
            fused_output = self.scale_fusion(torch.cat(hierarchical_outputs, dim=-1))
            
            # æœ€ç»ˆè¾“å‡º
            output = self.output_projection(fused_output)
            outputs.append(output)
        
        # å †å è¾“å‡º
        output_sequence = torch.stack(outputs, dim=1)
        
        # ç©ºé—´åµŒå…¥å¤„ç†ï¼ˆåœ¨åºåˆ—ç»“æŸåï¼‰
        if len(hidden_states) > 0:
            stacked_hidden = torch.stack([h.unsqueeze(1) for h in hidden_states], dim=2)  # [batch, 1, layers, hidden]
            stacked_hidden = stacked_hidden.squeeze(1)  # [batch, layers, hidden]
            
            # ä¸ºç©ºé—´åµŒå…¥å‡†å¤‡ä½ç½®
            last_positions = spatial_positions[:, -1].unsqueeze(1).expand(-1, self.num_layers)
            enhanced_hidden = self.spatial_embedding(stacked_hidden, last_positions)
            
            # æ›´æ–°æœ€ç»ˆéšè—çŠ¶æ€
            for i in range(self.num_layers):
                hidden_states[i] = enhanced_hidden[:, i, :]
        
        # è®¡ç®—æŸå¤±ç»„ä»¶
        if all_predictions and prediction_errors and all_uncertainties:
            prediction_loss = torch.mean(torch.stack(prediction_errors))
            uncertainty_loss = torch.mean(torch.stack(all_uncertainties))
            
            # å †å é¢„æµ‹ç»“æœ
            stacked_predictions = torch.stack(all_predictions, dim=1)
            stacked_uncertainties = torch.stack(all_uncertainties, dim=1)
            stacked_errors = torch.stack(prediction_errors, dim=1)
        else:
            # å¦‚æœæ²¡æœ‰é¢„æµ‹ç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼
            prediction_loss = torch.tensor(0.0, device=device)
            uncertainty_loss = torch.tensor(0.0, device=device)
            stacked_predictions = torch.zeros(batch_size, seq_len, self.prediction_size, device=device)
            stacked_uncertainties = torch.zeros(batch_size, seq_len, 1, device=device)
            stacked_errors = torch.zeros(batch_size, seq_len, 1, device=device)
        
        # è¿”å›ç»“æœ
        return output_sequence, {
            'predictions': stacked_predictions,
            'uncertainties': stacked_uncertainties,
            'prediction_errors': stacked_errors,
            'prediction_loss': prediction_loss,
            'uncertainty_loss': uncertainty_loss,
            'total_loss': (self.prediction_loss_weight * prediction_loss + 
                          self.uncertainty_loss_weight * uncertainty_loss),
            'hidden_states': hidden_states,
            'cell_states': cell_states
        }

# è®­ç»ƒå‡½æ•° - ä¿®å¤verboseå‚æ•°é—®é¢˜
class SeRNNTrainer:
    def __init__(self, model: SeRNN, learning_rate: float = 0.001):
        self.model = model
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
        
        # ä¿®å¤ï¼šç§»é™¤verboseå‚æ•°ï¼Œå› ä¸ºä¸æ˜¯æ‰€æœ‰PyTorchç‰ˆæœ¬éƒ½æ”¯æŒ
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=10, factor=0.5)
        
    def train_step(self, input_sequence: torch.Tensor, 
                   target_sequence: torch.Tensor,
                   spatial_positions: torch.Tensor) -> dict:
        """å•æ­¥è®­ç»ƒ"""
        self.model.train()
        self.optimizer.zero_grad()
        
        try:
            # å‰å‘ä¼ æ’­
            output_sequence, auxiliary_outputs = self.model(input_sequence, spatial_positions)
            
            # è®¡ç®—ä¸»è¦æŸå¤±
            reconstruction_loss = F.mse_loss(output_sequence, target_sequence)
            
            # æ€»æŸå¤±
            total_loss = (reconstruction_loss + 
                         auxiliary_outputs['prediction_loss'] * 0.5 +
                         auxiliary_outputs['uncertainty_loss'] * 0.1)
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            return {
                'total_loss': total_loss.item(),
                'reconstruction_loss': reconstruction_loss.item(),
                'prediction_loss': auxiliary_outputs['prediction_loss'].item(),
                'uncertainty_loss': auxiliary_outputs['uncertainty_loss'].item()
            }
            
        except Exception as e:
            print(f"è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
            print(f"è¾“å…¥å½¢çŠ¶: {input_sequence.shape}")
            print(f"ç›®æ ‡å½¢çŠ¶: {target_sequence.shape}")
            print(f"ç©ºé—´ä½ç½®å½¢çŠ¶: {spatial_positions.shape}")
            raise e
    
    def evaluate(self, input_sequence: torch.Tensor, 
                 target_sequence: torch.Tensor,
                 spatial_positions: torch.Tensor) -> dict:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        with torch.no_grad():
            output_sequence, auxiliary_outputs = self.model(input_sequence, spatial_positions)
            reconstruction_loss = F.mse_loss(output_sequence, target_sequence)
            
            return {
                'reconstruction_loss': reconstruction_loss.item(),
                'prediction_accuracy': 1.0 - torch.mean(auxiliary_outputs['prediction_errors']).item(),
                'mean_uncertainty': torch.mean(auxiliary_outputs['uncertainties']).item()
            }
    
    def step_scheduler(self, val_loss: float):
        """æ‰‹åŠ¨è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        old_lr = self.optimizer.param_groups[0]['lr']
        self.scheduler.step(val_loss)
        new_lr = self.optimizer.param_groups[0]['lr']
        
        if new_lr != old_lr:
            print(f"å­¦ä¹ ç‡è°ƒæ•´: {old_lr:.2e} -> {new_lr:.2e}")

# ä½¿ç”¨ç¤ºä¾‹ - é€‚é…384ç»´è¯åµŒå…¥
def create_sample_data(batch_size: int = 32, seq_len: int = 50, 
                      input_size: int = 384, spatial_dim: int = 1000):  # 384ç»´è¯åµŒå…¥
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    # ç”Ÿæˆè¾“å…¥åºåˆ— - æ¨¡æ‹Ÿ384ç»´è¯åµŒå…¥
    input_sequence = torch.randn(batch_size, seq_len, input_size) * 0.1  # è¾ƒå°çš„éšæœºå€¼
    
    # ç”Ÿæˆç©ºé—´ä½ç½®ï¼ˆéšæœºï¼‰
    spatial_positions = torch.randint(0, spatial_dim, (batch_size, seq_len))
    
    # ç›®æ ‡åºåˆ—ï¼ˆä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹ï¼‰
    target_sequence = torch.cat([input_sequence[:, 1:, :], 
                                torch.randn(batch_size, 1, input_size) * 0.1], dim=1)
    
    return input_sequence, target_sequence, spatial_positions

# ä¸»å‡½æ•°
def main():
    # æ¨¡å‹å‚æ•° - é€‚é…384ç»´è¯åµŒå…¥
    config = {
        'input_size': 384,        # 384ç»´è¯åµŒå…¥
        'hidden_size': 384,       # ä¿æŒä¸€è‡´çš„éšè—ç»´åº¦
        'spatial_dim': 1000,      # æ‰©å¤§ç©ºé—´ç»´åº¦
        'num_layers': 3,
        'dropout': 0.1,
        'hierarchical_scales': [1, 4, 16],
        'prediction_size': 384    # é¢„æµ‹è¾“å‡ºä¹Ÿæ˜¯384ç»´
    }
    
    print("åˆ›å»ºå¢å¼ºç‰ˆseRNNæ¨¡å‹ï¼ˆ384ç»´è¯åµŒå…¥ï¼‰...")
    model = SeRNN(**config)
    trainer = SeRNNTrainer(model, learning_rate=0.001)
    
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒå¾ªç¯
    num_epochs = 50  # å‡å°‘epochæ•°ç”¨äºæµ‹è¯•
    print(f"å¼€å§‹è®­ç»ƒ {num_epochs} ä¸ªepoch...")
    
    best_loss = float('inf')
    
    for epoch in range(num_epochs):
        try:
            # ç”Ÿæˆè®­ç»ƒæ•°æ®
            input_seq, target_seq, spatial_pos = create_sample_data()
            
            # è®­ç»ƒæ­¥éª¤
            train_metrics = trainer.train_step(input_seq, target_seq, spatial_pos)
            
            # è¯„ä¼°å’Œå­¦ä¹ ç‡è°ƒæ•´
            if epoch % 5 == 0:
                eval_metrics = trainer.evaluate(input_seq, target_seq, spatial_pos)
                current_loss = eval_metrics['reconstruction_loss']
                
                # è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
                trainer.step_scheduler(current_loss)
                
                # æ›´æ–°æœ€ä½³æŸå¤±
                if current_loss < best_loss:
                    best_loss = current_loss
                    print(f"â˜… æ–°çš„æœ€ä½³æŸå¤±: {best_loss:.6f}")
                
                print(f"Epoch {epoch:3d}:")
                print(f"  Train Loss: {train_metrics['total_loss']:.6f}")
                print(f"  Eval Loss: {eval_metrics['reconstruction_loss']:.6f}")
                print(f"  Prediction Accuracy: {eval_metrics['prediction_accuracy']:.4f}")
                print(f"  Mean Uncertainty: {eval_metrics['mean_uncertainty']:.6f}")
                print(f"  Learning Rate: {trainer.optimizer.param_groups[0]['lr']:.2e}")
                print("-" * 50)
                
        except Exception as e:
            print(f"è®­ç»ƒåœ¨epoch {epoch}æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            break
    
    print("è®­ç»ƒæˆåŠŸå®Œæˆ!")
    return model, trainer

if __name__ == "__main__":
    try:
        model, trainer = main()
        print("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': trainer.optimizer.state_dict(),
            'model_config': {
                'input_size': 384,
                'hidden_size': 384,
                'spatial_dim': 1000,
                'num_layers': 3,
                'dropout': 0.1,
                'hierarchical_scales': [1, 4, 16],
                'prediction_size': 384
            }
        }, 'enhanced_sernn_384dim.pth')
        
        print("æ¨¡å‹å·²ä¿å­˜åˆ° 'enhanced_sernn_384dim.pth'")
        
        # åŠ è½½æ¨¡å‹ç¤ºä¾‹
        print("\nåŠ è½½æ¨¡å‹ç¤ºä¾‹:")
        checkpoint = torch.load('enhanced_sernn_384dim.pth')
        config = checkpoint['model_config']
        loaded_model = SeRNN(**config)
        loaded_model.load_state_dict(checkpoint['model_state_dict'])
        print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
