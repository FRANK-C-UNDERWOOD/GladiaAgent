"""
ğŸ§  PredictiveCodingAgent æ¨¡å—
Author: DOCTOR + æ­Œè•¾è’‚å¨… (2025)

æœ¬æ¨¡å—å®ç°ä¸€ä¸ªå…·å¤‡æ„ŸçŸ¥é¢„æµ‹ã€è¯¯å·®åå‘ä¿®æ­£ä¸è®°å¿†æœºåˆ¶çš„åŸºç¡€é¢„æµ‹ç¼–ç  Agentã€‚
è¯¥ç»“æ„å¯ç”¨äºä¸»åŠ¨æ„ŸçŸ¥ã€å¼‚å¸¸æ£€æµ‹ã€æ–°å¥‡æ€§è®°å¿†ã€è‡ªé€‚åº”æ§åˆ¶ç­‰åœºæ™¯ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. encode_input()   - å°†è¾“å…¥ç¼–ç ä¸ºéšçŠ¶æ€å‘é‡
2. decode_prediction() - ä»éšçŠ¶æ€ç”Ÿæˆé¢„æµ‹
3. forward_predict()   - æ‰§è¡Œå¤šè½®é¢„æµ‹-è¯¯å·®-ä¿®æ­£é—­ç¯æ¨ç†
4. update_memory()     - å°†é«˜é¢„æµ‹è¯¯å·®çš„è¾“å…¥è®°å…¥è®°å¿†åº“
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import warnings

class ContinuousTimeNeuron(nn.Module):
    """
    è¿ç»­æ—¶é—´ç¥ç»å…ƒ - ä¿®å¤æ‰¹æ¬¡ç»´åº¦é—®é¢˜ç‰ˆæœ¬
    åŸºäºé¢„æµ‹ç¼–ç ç†è®ºçš„ç”Ÿç‰©å¯å‘ç¥ç»å…ƒæ¨¡å‹
    """
    def __init__(self, input_size: int = 64, hidden_size: int = 128, 
                 memory_capacity: int = 1000, tau: float = 0.1, 
                 learning_rate: float = 0.01):
        super(ContinuousTimeNeuron, self).__init__()
        
        # ç¡®ä¿ç»´åº¦å‚æ•°
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.memory_capacity = memory_capacity
        self.tau = tau  # æ—¶é—´å¸¸æ•°
        self.learning_rate = learning_rate
        
        # æƒé‡çŸ©é˜µ - ç»´åº¦æ˜ç¡®å®šä¹‰
        self.W_input = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)
        self.W_recurrent = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)
        self.W_output = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)
        
        # åç½®é¡¹
        self.bias_hidden = nn.Parameter(torch.zeros(hidden_size))
        self.bias_output = nn.Parameter(torch.zeros(input_size))
        
        # çŠ¶æ€å˜é‡ - ä¿®å¤ï¼šä¸å†ä¿å­˜æ‰¹æ¬¡çŠ¶æ€ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­æ—¶é‡æ–°åˆå§‹åŒ–
        self.hidden_size_dim = hidden_size
        
        # è®°å¿†å­˜å‚¨
        self.memory_bank = []
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Xavieråˆå§‹åŒ–æƒé‡"""
        nn.init.xavier_uniform_(self.W_input)
        nn.init.xavier_uniform_(self.W_recurrent)
        nn.init.xavier_uniform_(self.W_output)
    
    def _init_hidden_state(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """åˆå§‹åŒ–éšè—çŠ¶æ€"""
        return torch.zeros(batch_size, self.hidden_size, device=device, dtype=torch.float32)
    
    def forward(self, x: torch.Tensor, hidden_state: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥å¼ é‡ (batch_size, input_size)
            hidden_state: å¯é€‰çš„åˆå§‹éšè—çŠ¶æ€ (batch_size, hidden_size)
        Returns:
            prediction: é¢„æµ‹è¾“å‡º (batch_size, input_size)
            new_hidden_state: æ–°çš„éšè—çŠ¶æ€ (batch_size, hidden_size)
        """
        # ç»´åº¦æ£€æŸ¥
        if x.dim() == 1:
            x = x.unsqueeze(0)
        
        batch_size = x.size(0)
        device = x.device
        
        assert x.size(1) == self.input_size, \
            f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.input_size}, å¾—åˆ° {x.size(1)}"
        
        # åˆå§‹åŒ–æˆ–éªŒè¯éšè—çŠ¶æ€
        if hidden_state is None:
            current_hidden_state = self._init_hidden_state(batch_size, device)
        else:
            assert hidden_state.size() == (batch_size, self.hidden_size), \
                f"éšè—çŠ¶æ€ç»´åº¦ä¸åŒ¹é…: æœŸæœ› ({batch_size}, {self.hidden_size}), å¾—åˆ° {hidden_state.size()}"
            current_hidden_state = hidden_state
        
        # è®¡ç®—æ–°çš„éšè—çŠ¶æ€
        input_contribution = torch.matmul(x, self.W_input.t())
        recurrent_contribution = torch.matmul(current_hidden_state, self.W_recurrent.t())
        
        # è¿ç»­æ—¶é—´åŠ¨åŠ›å­¦ (ç®€åŒ–çš„æ¬§æ‹‰æ–¹æ³•)
        dh_dt = (-current_hidden_state + torch.tanh(
            input_contribution + recurrent_contribution + self.bias_hidden
        )) / self.tau
        
        new_hidden_state = current_hidden_state + dh_dt * 0.01  # æ—¶é—´æ­¥é•¿
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = torch.matmul(new_hidden_state, self.W_output.t()) + self.bias_output
        
        return prediction, new_hidden_state
    
    def update_memory(self, memory_vector: torch.Tensor):
        """æ›´æ–°è®°å¿†åº“"""
        if len(self.memory_bank) >= self.memory_capacity:
            self.memory_bank.pop(0)
        self.memory_bank.append(memory_vector.detach().cpu())

class PredictiveCodingAgent(nn.Module):
    """
    é¢„æµ‹ç¼–ç æ™ºèƒ½ä½“ - ä¿®å¤æ‰¹æ¬¡ç»´åº¦é—®é¢˜ç‰ˆæœ¬
    æ•´åˆå¤šä¸ªè¿ç»­æ—¶é—´ç¥ç»å…ƒçš„é¢„æµ‹ç¼–ç ç½‘ç»œ
    """
    def __init__(self, num_inputs: int = 384, encoding_dim: int = 64, 
                 hidden_size: int = 128, num_neurons: int = 4,
                 memory_capacity: int = 1000):
        super(PredictiveCodingAgent, self).__init__()
        
        # ç½‘ç»œå‚æ•°
        self.num_inputs = num_inputs
        self.encoding_dim = encoding_dim
        self.hidden_size = hidden_size
        self.num_neurons = num_neurons
        self.memory_capacity = memory_capacity
        
        # ç¼–ç å™¨ï¼š384 -> 64
        self.encoder = nn.Sequential(
            nn.Linear(num_inputs, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, encoding_dim),
            nn.Tanh()  # é™åˆ¶ç¼–ç èŒƒå›´
        )
        
        # è¿ç»­æ—¶é—´ç¥ç»å…ƒç»„
        self.neurons = nn.ModuleList([
            ContinuousTimeNeuron(
                input_size=encoding_dim,  # ä¸ç¼–ç å™¨è¾“å‡ºå¯¹é½
                hidden_size=hidden_size,
                memory_capacity=memory_capacity
            ) for _ in range(num_neurons)
        ])
        
        # è§£ç å™¨ï¼š64 -> 384
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_inputs)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ç”¨äºèåˆå¤šä¸ªç¥ç»å…ƒè¾“å‡º
        self.attention = nn.MultiheadAttention(
            embed_dim=encoding_dim,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm1 = nn.LayerNorm(encoding_dim)
        self.layer_norm2 = nn.LayerNorm(encoding_dim)
        
        # è®°å¿†ç¼–ç å‚æ•°
        self.memory_embed_dim = 30  # å›ºå®šè®°å¿†å‘é‡ç»´åº¦
        self.memory_projection = nn.Linear(self.memory_embed_dim, encoding_dim)
        
        # ç¥ç»å…ƒçŠ¶æ€ç®¡ç† - ä¿®å¤ï¼šä½¿ç”¨å­—å…¸å­˜å‚¨ä¸åŒå½¢çŠ¶çš„çŠ¶æ€
        self.neuron_states = {}
    
    def _get_neuron_states(self, batch_size: int, device: torch.device) -> List[torch.Tensor]:
        """è·å–æˆ–åˆ›å»ºç¥ç»å…ƒçŠ¶æ€"""
        key = f"{batch_size}_{device}"
        if key not in self.neuron_states:
            self.neuron_states[key] = [
                torch.zeros(batch_size, self.hidden_size, device=device)
                for _ in range(self.num_neurons)
            ]
        return self.neuron_states[key]
    
    def encode_input(self, subject: Any, predicate: Any, obj: Any) -> torch.Tensor:
        """
        ç¼–ç ä¸‰å…ƒç»„è¾“å…¥ä¸ºå›ºå®šç»´åº¦çš„è®°å¿†å‘é‡
        Args:
            subject, predicate, obj: ä¸‰å…ƒç»„å…ƒç´ 
        Returns:
            encoded_memory: å›ºå®š30ç»´çš„è®°å¿†å‘é‡
        """
        def pad_or_truncate_encode(text: Any, length: int = 10) -> List[float]:
            """å°†æ–‡æœ¬ç¼–ç ä¸ºå›ºå®šé•¿åº¦çš„æ•°å€¼å‘é‡"""
            text_str = str(text)[:length]
            text_str = text_str.ljust(length, ' ')  # ç”¨ç©ºæ ¼å¡«å……åˆ°å›ºå®šé•¿åº¦
            return [float(ord(c)) / 127.0 for c in text_str]  # å½’ä¸€åŒ–åˆ°[-1,1]
        
        # ç¼–ç æ¯ä¸ªå…ƒç´ ä¸º10ç»´
        s_enc = pad_or_truncate_encode(subject, 10)
        p_enc = pad_or_truncate_encode(predicate, 10)
        o_enc = pad_or_truncate_encode(obj, 10)
        
        # ç»„åˆä¸º30ç»´å‘é‡
        memory_vector = torch.tensor(s_enc + p_enc + o_enc, dtype=torch.float32)
        
        # ç»´åº¦æ£€æŸ¥
        assert memory_vector.shape[0] == self.memory_embed_dim, \
            f"è®°å¿†å‘é‡ç»´åº¦é”™è¯¯: æœŸæœ› {self.memory_embed_dim}, å¾—åˆ° {memory_vector.shape[0]}"
        
        return memory_vector
    
    def predict_with_memory(self, memory_vector: torch.Tensor, 
                          context: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        åŸºäºè®°å¿†å‘é‡ç”Ÿæˆé¢„æµ‹
        Args:
            memory_vector: è®°å¿†å‘é‡ (batch_size, memory_embed_dim)
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ (batch_size, num_inputs)
        Returns:
            prediction: é¢„æµ‹ç»“æœ (batch_size, num_inputs)
        """
        if memory_vector.dim() == 1:
            memory_vector = memory_vector.unsqueeze(0)
        
        batch_size = memory_vector.size(0)
        device = memory_vector.device
        
        # å°†è®°å¿†å‘é‡æŠ•å½±åˆ°ç¼–ç ç»´åº¦
        encoded_memory = self.memory_projection(memory_vector)  # (batch_size, encoding_dim)
        
        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯
        if context is not None:
            # ç»´åº¦æ£€æŸ¥
            assert context.size(-1) == self.num_inputs, \
                f"ä¸Šä¸‹æ–‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.num_inputs}, å¾—åˆ° {context.size(-1)}"
            
            # ç¼–ç ä¸Šä¸‹æ–‡
            encoded_context = self.encoder(context)  # (batch_size, encoding_dim)
            
            # èåˆè®°å¿†å’Œä¸Šä¸‹æ–‡
            encoded_input = (encoded_memory + encoded_context) / 2.0
        else:
            encoded_input = encoded_memory
        
        # å¤šç¥ç»å…ƒå¤„ç† - ä¿®å¤ï¼šæ¯æ¬¡éƒ½é‡æ–°åˆå§‹åŒ–çŠ¶æ€
        neuron_outputs = []
        
        for neuron in self.neurons:
            # æ¯æ¬¡è°ƒç”¨éƒ½ä½¿ç”¨æ–°çš„éšè—çŠ¶æ€
            prediction, _ = neuron(encoded_input, hidden_state=None)
            neuron_outputs.append(prediction.unsqueeze(1))  # (batch_size, 1, encoding_dim)
        
        # å †å ç¥ç»å…ƒè¾“å‡º
        stacked_outputs = torch.cat(neuron_outputs, dim=1)  # (batch_size, num_neurons, encoding_dim)
        
        # æ³¨æ„åŠ›æœºåˆ¶èåˆ
        attended_output, attention_weights = self.attention(
            stacked_outputs, stacked_outputs, stacked_outputs
        )  # (batch_size, num_neurons, encoding_dim)
        
        # å¹³å‡æ± åŒ–
        fused_output = attended_output.mean(dim=1)  # (batch_size, encoding_dim)
        
        # å±‚å½’ä¸€åŒ–
        fused_output = self.layer_norm1(fused_output)
        
        # æ®‹å·®è¿æ¥
        fused_output = fused_output + encoded_input
        fused_output = self.layer_norm2(fused_output)
        
        # è§£ç 
        prediction = self.decoder(fused_output)  # (batch_size, num_inputs)
        
        return prediction
    
    def forward(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å®Œæ•´çš„å‰å‘ä¼ æ’­ - ä¿®å¤æ‰¹æ¬¡ç»´åº¦é—®é¢˜ç‰ˆæœ¬
        Args:
            inputs: è¾“å…¥å¼ é‡ (batch_size, seq_len, num_inputs) æˆ– (batch_size, num_inputs)
        Returns:
            outputs: è¾“å‡ºé¢„æµ‹ (batch_size, seq_len, num_inputs)
            total_loss: æ€»æŸå¤±
            metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        # ç»´åº¦å¤„ç†
        if inputs.dim() == 2:
            inputs = inputs.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        
        batch_size, seq_len, input_dim = inputs.shape
        device = inputs.device
        
        # ç»´åº¦æ£€æŸ¥
        assert input_dim == self.num_inputs, \
            f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.num_inputs}, å¾—åˆ° {input_dim}"
        
        # ç¼–ç è¾“å…¥
        inputs_reshaped = inputs.view(-1, input_dim)  # (batch_size*seq_len, num_inputs)
        encoded_inputs = self.encoder(inputs_reshaped)  # (batch_size*seq_len, encoding_dim)
        encoded_inputs = encoded_inputs.view(batch_size, seq_len, self.encoding_dim)
        
        # åˆå§‹åŒ–ç¥ç»å…ƒçŠ¶æ€
        neuron_states = [
            torch.zeros(batch_size, self.hidden_size, device=device)
            for _ in range(self.num_neurons)
        ]
        
        # é€æ—¶é—´æ­¥å¤„ç†
        outputs = []
        prediction_errors = []
        
        for t in range(seq_len):
            current_input = encoded_inputs[:, t, :]  # (batch_size, encoding_dim)
            
            # å¤šç¥ç»å…ƒå¤„ç†
            step_outputs = []
            step_errors = []
            
            for i, neuron in enumerate(self.neurons):
                prediction, new_state = neuron(current_input, neuron_states[i])
                neuron_states[i] = new_state  # æ›´æ–°çŠ¶æ€
                
                # è®¡ç®—é¢„æµ‹è¯¯å·®
                error = F.mse_loss(prediction, current_input, reduction='none')
                
                step_outputs.append(prediction.unsqueeze(1))
                step_errors.append(error.mean(dim=1, keepdim=True))
            
            # èåˆç¥ç»å…ƒè¾“å‡º
            stacked_outputs = torch.cat(step_outputs, dim=1)  # (batch_size, num_neurons, encoding_dim)
            
            # æ³¨æ„åŠ›èåˆ
            attended_output, _ = self.attention(
                stacked_outputs, stacked_outputs, stacked_outputs
            )
            fused_output = attended_output.mean(dim=1)  # (batch_size, encoding_dim)
            
            # å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥
            fused_output = self.layer_norm1(fused_output)
            fused_output = fused_output + current_input
            fused_output = self.layer_norm2(fused_output)
            
            outputs.append(fused_output.unsqueeze(1))
            prediction_errors.extend(step_errors)
        
        # ç»„åˆè¾“å‡º
        encoded_outputs = torch.cat(outputs, dim=1)  # (batch_size, seq_len, encoding_dim)
        
        # è§£ç 
        encoded_outputs_reshaped = encoded_outputs.view(-1, self.encoding_dim)
        decoded_outputs = self.decoder(encoded_outputs_reshaped)
        final_outputs = decoded_outputs.view(batch_size, seq_len, self.num_inputs)
        
        # è®¡ç®—æŸå¤±
        reconstruction_loss = F.mse_loss(final_outputs, inputs)
        prediction_loss = torch.stack(prediction_errors).mean() if prediction_errors else torch.tensor(0.0, device=device)
        total_loss = reconstruction_loss + 0.1 * prediction_loss
        
        # æ€§èƒ½æŒ‡æ ‡
        metrics = {
            'reconstruction_loss': reconstruction_loss,
            'prediction_loss': prediction_loss,
            'total_loss': total_loss,
            'input_norm': inputs.norm(dim=-1).mean(),
            'output_norm': final_outputs.norm(dim=-1).mean()
        }
        
        return final_outputs, total_loss, metrics
    
    def get_memory_summary(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ä½¿ç”¨æ‘˜è¦"""
        memory_info = {}
        for i, neuron in enumerate(self.neurons):
            memory_info[f'neuron_{i}'] = {
                'memory_count': len(neuron.memory_bank),
                'memory_capacity': neuron.memory_capacity,
                'utilization': len(neuron.memory_bank) / neuron.memory_capacity
            }
        return memory_info
    
    def reset_states(self):
        """é‡ç½®æ‰€æœ‰ç¥ç»å…ƒçŠ¶æ€"""
        self.neuron_states.clear()

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ¨¡å‹
    model = PredictiveCodingAgent(
        num_inputs=384,
        encoding_dim=64,
        hidden_size=128,
        num_neurons=4,
        memory_capacity=1000
    ).to(device)
    
    print("ğŸ” æ¨¡å‹ç»“æ„:")
    print(f"  å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„åŸºæœ¬åŠŸèƒ½
    print("\nğŸ“Š æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°:")
    
    for batch_size in [1, 4, 8]:
        seq_len = 32
        test_input = torch.randn(batch_size, seq_len, 384).to(device)
        
        print(f"\n  æ‰¹æ¬¡å¤§å° {batch_size}:")
        print(f"    è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs, loss, metrics = model(test_input)
        
        print(f"    âœ… è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
        print(f"    âœ… æ€»æŸå¤±: {loss.item():.6f}")
    
    # æµ‹è¯•è®°å¿†ç¼–ç å’Œé¢„æµ‹
    print("\nğŸ§  æµ‹è¯•è®°å¿†åŠŸèƒ½:")
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„è®°å¿†é¢„æµ‹
    for batch_size in [1, 3]:
        memory_vec = model.encode_input("subject_example", "predicate_example", "object_example")
        memory_batch = memory_vec.unsqueeze(0).repeat(batch_size, 1).to(device)
        
        print(f"\n  æ‰¹æ¬¡å¤§å° {batch_size}:")
        print(f"    è®°å¿†å‘é‡å½¢çŠ¶: {memory_batch.shape}")
        
        # åŸºäºè®°å¿†çš„é¢„æµ‹
        with torch.no_grad():
            memory_prediction = model.predict_with_memory(memory_batch)
        
        print(f"    âœ… è®°å¿†é¢„æµ‹å½¢çŠ¶: {memory_prediction.shape}")
    
    # è®°å¿†ä½¿ç”¨æƒ…å†µ
    memory_summary = model.get_memory_summary()
    print(f"\nğŸ“ˆ è®°å¿†ä½¿ç”¨æƒ…å†µ: {memory_summary}")
    
    print("\nğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤§å°æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å°±ç»ªã€‚")
    
    # æ¸…ç†çŠ¶æ€
    model.reset_states()
    print("ğŸ§¹ çŠ¶æ€å·²æ¸…ç†")
